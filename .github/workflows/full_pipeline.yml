name: Full SF Crime Pipeline

on:
  schedule:
    - cron: "0 14,15 * * *"   # SF 07:00: 14:00 UTC (yaz), 15:00 UTC (kış)
  workflow_dispatch:
    inputs:
      persist:
        description: "Sonuçları nasıl saklayalım?"
        type: choice
        options: [artifact, commit, none]
        default: artifact
      force:
        description: "Manuel tetiklemede 07:00 kapısını YOK SAY"
        type: boolean
        default: true
      top_k:
        description: "Stacking: her saat dilimi için önerilecek GEOID sayısı"
        default: "50"

permissions:
  actions: read
  contents: write

concurrency:
  group: full-pipeline-${{ github.ref }}
  cancel-in-progress: true

env:
  CRIME_DATA_DIR: crime_prediction_data_pre
  GEOID_LEN: "11"

  BACKFILL_DAYS: "0"
  SF_SODA_FETCH_STRATEGY: ${{ vars.SF_SODA_FETCH_STRATEGY || 'bulk' }}
  SF_SODA_PAGE_LIMIT:    ${{ vars.SF_SODA_PAGE_LIMIT    || '50000' }}
  SF_SODA_MAX_PAGES:     ${{ vars.SF_SODA_MAX_PAGES     || '100' }}

  SF911_API_URL:       ${{ vars.SF911_API_URL       || 'https://data.sfgov.org/resource/2zdj-bwza.json' }}
  SF911_AGENCY_FILTER: ${{ vars.SF911_AGENCY_FILTER || 'agency like "%Police%"' }}
  SF911_API_TOKEN:     ${{ secrets.SF911_API_TOKEN }}
  SOCS_APP_TOKEN:      ${{ secrets.SOCS_APP_TOKEN }}

  PATROL_TOP_K: ${{ github.event.inputs.top_k || '50' }}

  PATROL_HORIZON_DAYS: "3"
  WX_LOCATION: "san francisco"
  WX_UNIT: "us"

  ACS_YEAR: ${{ vars.ACS_YEAR || 'LATEST' }}
  DEMOG_WHITELIST: ${{ vars.DEMOG_WHITELIST || '' }}
  CENSUS_GEO_LEVEL: ${{ vars.CENSUS_GEO_LEVEL || 'auto' }}

  ARTIFACT_NAME: sf-crime-pipeline-output
  ARTIFACT_ZIP: crime_prediction_data_pre.zip   # istenen zip adı (cem5113/crime_prediction_data_pre konseptine uygun)

jobs:
  run:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout (with LFS)
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Ensure LFS objects
        run: |
          git lfs install
          git lfs pull
          echo "Repo root:"; ls -lah
          echo "crime_prediction_data_pre:"; ls -lah crime_prediction_data_pre || true
          echo "CRIME_DATA_DIR:"; ls -lah "${CRIME_DATA_DIR}" || true

      - name: Ensure data dir
        run: mkdir -p "${CRIME_DATA_DIR}"

      # --- SF 07:00 kapısı (TZ ayarla + saat kontrolü) ---
      - name: Set runner timezone to America/Los_Angeles
        uses: szenius/set-timezone@v2.0
        with:
          timezoneLinux: "America/Los_Angeles"

      - name: Gate by local SF time == 07 (bypassable)
        id: gate
        run: |
          now="$(date)"; echo "Runner local time: $now"; echo "RUN_LOCAL_TIME=$now" >> $GITHUB_ENV
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.force }}" = "true" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT; exit 0; fi
          if [ "$(date +%H)" = "07" ]; then echo "proceed=true" >> $GITHUB_OUTPUT; else echo "proceed=false" >> $GITHUB_OUTPUT; fi

      - name: Skip summary (outside 07:00 and not forced)
        if: ${{ steps.gate.outputs.proceed != 'true' }}
        run: |
          {
            echo "## SF Crime Pipeline"
            echo ""
            echo "- Çalışma zamanı (SF): **$(date)**"
            echo "- Not: 07:00 kapısı nedeniyle adımlar atlandı. Manuel tetiklerken \`force=true\` verin."
          } >> $GITHUB_STEP_SUMMARY

      - name: System deps for rtree (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: sudo apt-get update && sudo apt-get install -y libspatialindex-dev

      - name: Set up Python 3.11
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install -U "geopandas==1.0.1" "shapely==2.0.4" "pyproj==3.6.1" "pyogrio==0.9.0" "rtree==1.3.0" pyarrow

      - name: Geo stack smoke test
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import geopandas, shapely, pyproj, pyogrio, pandas
          print("geopandas", geopandas.__version__)
          print("shapely", shapely.__version__)
          print("pyproj", pyproj.__version__)
          print("pyogrio", pyogrio.__version__)
          print("pandas", pandas.__version__)
          PY

      # >>> Prefetch: önce *_y.csv, bulunamazsa orijinal .csv
      - name: 00) Prefetch sf_crime (y → fallback)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: ${{ env.ARTIFACT_NAME }}
          WORKFLOW_NAME: Full SF Crime Pipeline
          OUT_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"
          sudo apt-get update -y >/dev/null; sudo apt-get install -y unzip >/dev/null
          if ! command -v gh >/dev/null; then type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh; fi
          try_fetch () {
            local wanted="$1"
            mkdir -p _prev; local found=""
            for RID in $(gh run list -R "${GITHUB_REPOSITORY}" --workflow "${WORKFLOW_NAME}" --status success -L 10 --json databaseId -q '.[].databaseId' || true); do
              rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
              if gh run download -R "${GITHUB_REPOSITORY}" "$RID" -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
                F=$(find "_prev/$RID" -type f -iname "${wanted}" | head -n1 || true)
                if [ -n "${F:-}" ]; then cp -f "$F" "${OUT_DIR}/${wanted}"; echo "✅ Prefetch: ${wanted}"; found="yes"; break; fi
              fi
            done
            [ -n "${found}" ]
          }
          try_fetch sf_crime_y.csv || echo "ℹ️ sf_crime_y.csv yok; scriptler orijinal .csv ile devam edecek."

      # ---- PIPELINE ----
      - name: 01) Suç tabanı ve grid → sf_crime.csv + gridler
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_crime.py

      - name: Restore 911 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
          key: 911-${{ runner.os }}-${{ github.run_number }}
          restore-keys: |
            911-${{ runner.os }}-

      - name: Prefetch 911 (y → fallback)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: ${{ env.ARTIFACT_NAME }}
          WORKFLOW_NAME: Full SF Crime Pipeline
          OUT_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"
          sudo apt-get update -y >/dev/null; sudo apt-get install -y unzip >/dev/null
          if ! command -v gh >/dev/null; then type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh; fi
          fetch_one () {
            local wanted="$1"
            mkdir -p _prev; local found=""
            for RID in $(gh run list -R "${GITHUB_REPOSITORY}" --workflow "${WORKFLOW_NAME}" --status success -L 10 --json databaseId -q '.[].databaseId' || true); do
              rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
              if gh run download -R "${GITHUB_REPOSITORY}" "$RID" -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
                F=$(find "_prev/$RID" -type f -iname "${wanted}" | head -n1 || true)
                if [ -n "${F:-}" ]; then cp -f "$F" "${OUT_DIR}/${wanted}"; echo "✅ Prefetch: ${wanted}"; found="yes"; break; fi
              fi
            done
            [ -n "${found}" ]
          }
          fetch_one sf_911_last_5_year_y.csv || echo "ℹ️ *_y bulunamadı; update_911.py orijinal .csv akışıyla devam eder."

      - name: 02) 911 → sf_crime_01.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SF911_API_URL:         ${{ env.SF911_API_URL }}
          SF911_AGENCY_FILTER:   ${{ env.SF911_AGENCY_FILTER }}
          SF911_API_TOKEN:       ${{ env.SF911_API_TOKEN }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
        run: |
          set -e
          python -u update_911.py

      - name: Restore 311 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv
          key: 311-${{ runner.os }}-${{ github.run_number }}
          restore-keys: |
            311-${{ runner.os }}-

      - name: Prefetch 311 (y → fallback)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: ${{ env.ARTIFACT_NAME }}
          WORKFLOW_NAME: Full SF Crime Pipeline
          OUT_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"
          sudo apt-get update -y >/dev/null; sudo apt-get install -y unzip >/dev/null
          if ! command -v gh >/dev/null; then type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh; fi
          fetch_one () {
            local wanted="$1"
            mkdir -p _prev; local found=""
            for RID in $(gh run list -R "${GITHUB_REPOSITORY}" --workflow "${WORKFLOW_NAME}" --status success -L 10 --json databaseId -q '.[].databaseId' || true); do
              rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
              if gh run download -R "${GITHUB_REPOSITORY}" "$RID" -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
                F=$(find "_prev/$RID" -type f -iname "${wanted}" | head -n1 || true)
                if [ -n "${F:-}" ]; then cp -f "$F" "${OUT_DIR}/${wanted}"; echo "✅ Prefetch: ${wanted}"; found="yes"; break; fi
              fi
            done
            [ -n "${found}" ]
          }
          fetch_one sf_311_last_5_years_y.csv || echo "ℹ️ *_y bulunamadı; update_311.py orijinal .csv ile devam eder."

      - name: 03) 311 → sf_crime_02.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
        run: |
          set -e
          if [ -f update_311.py ]; then python -u update_311.py; else python -u scripts/update_311.py; fi

      - name: Ensure population CSV (local file)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          mkdir -p "${CRIME_DATA_DIR}"
          DEST="${CRIME_DATA_DIR}/sf_population.csv"
          CANDIDATES=("${CRIME_DATA_DIR}/sf_population.csv" "sf_population.csv" "data/sf_population.csv" "inputs/sf_population.csv")
          FOUND=""
          for p in "${CANDIDATES[@]}"; do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then echo "ℹ️ Zaten hedefte: $DEST"; else cp -f "$p" "$DEST"; echo "✅ Copied: $p → $DEST"; fi
              FOUND="yes"; break
            fi
          done
          if [ -z "$FOUND" ]; then echo "❌ sf_population.csv bulunamadı."; exit 2; fi
          echo "---- sf_population.csv (head) ----"; head -n 5 "$DEST" || true

      - name: (Opsiyonel) sf_population.csv başlık normalize
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import os, pandas as pd
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "sf_population.csv")
          df = pd.read_csv(p)
          low = {c.lower(): c for c in df.columns}
          if 'geoid' not in low and 'geography_id' in low: df.rename(columns={low['geography_id']: 'GEOID'}, inplace=True)
          for cand in ['population','total_population','b01003_001e','estimate','total','value']:
              if cand in low:
                  if 'population' not in df.columns: df.rename(columns={low[cand]: 'population'}, inplace=True)
                  break
          df.to_csv(p, index=False); print("Normalized headers:", df.columns.tolist())
          PY

      - name: 04) Nüfus → sf_crime_03.csv (demografi + nüfus)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          POPULATION_PATH:  ${{ env.CRIME_DATA_DIR }}/sf_population.csv
          CENSUS_GEO_LEVEL: ${{ env.CENSUS_GEO_LEVEL }}
          ACS_YEAR:         ${{ env.ACS_YEAR }}
          DEMOG_WHITELIST:  ${{ env.DEMOG_WHITELIST }}
        run: |
          set -e
          python -u update_population.py

      - name: 05) Otobüs → sf_crime_04.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_bus.py

      - name: 06) Tren (BART) → sf_crime_05.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_train.py

      - name: 07) POI zenginleştirme → sf_crime_06.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_poi.py ]; then python -u update_poi.py;
          elif [ -f pipeline_make_sf_crime_06.py ]; then python -u pipeline_make_sf_crime_06.py;
          else echo "POI adımı bulunamadı"; exit 2; fi

      - name: 08) Polis & Devlet binaları → sf_crime_07.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_police_gov.py ]; then python -u update_police_gov.py;
          elif [ -f scripts/enrich_police_gov_06_to_07.py ]; then python -u scripts/enrich_police_gov_06_to_07.py;
          else echo "Polis/Gov adımı bulunamadı"; exit 2; fi

      - name: 09) sf_crime_08_static üretimi (07 → 08_static)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python - <<'PY'
          import os, pandas as pd
          from pathlib import Path
          CRIME_DIR = os.getenv("CRIME_DATA_DIR","crime_prediction_data_pre")
          candidates = [Path(CRIME_DIR)/"sf_crime_07.csv", Path("crime_prediction_data_pre")/"sf_crime_07.csv",
                        Path("crime_data")/"sf_crime_07.csv", Path("sf_crime_07.csv"), Path("outputs")/"sf_crime_07.csv"]
          src_path = next((p for p in candidates if p.exists()), None)
          if src_path is None: raise FileNotFoundError("sf_crime_07.csv not found")
          dst = str(Path(CRIME_DIR)/"sf_crime_08_static.csv")
          try:
              from scripts.common import clean_and_save_crime_08
              clean_and_save_crime_08(str(src_path), dst); print("✅ sf_crime_08_static hazır →", dst)
          except Exception as e:
              print(f"⚠️ clean_and_save_crime_08 kullanılamadı ({e}); passthrough yazılıyor.")
              pd.read_csv(src_path, low_memory=False).to_csv(dst, index=False); print("✅ Passthrough 08_static yazıldı →", dst)
          PY

      - name: 10.5) neighbors.csv (kullan/normalize/üret + kalite kontrol)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_events_enriched.csv
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          NEIGHBOR_STRATEGY: queen
          MIN_NEIGHBOR_DEG: "3"
          KNN_K: "5"
          NEIGHBOR_POLY: ${{ env.CRIME_DATA_DIR }}/sf_grid.geojson
        run: |
          set -euo pipefail
          DEST="${CRIME_DATA_DIR}/neighbors.csv"; mkdir -p "${CRIME_DATA_DIR}"
          echo "→ hedef: $DEST"
          for p in "${CRIME_DATA_DIR}/neighbors.csv" "crime_prediction_data_pre/neighbors.csv" "neighbors.csv" "data/neighbors.csv" "inputs/neighbors.csv"
          do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then echo "ℹ️ Zaten hedefte: $DEST"; else cp -f "$p" "$DEST"; echo "✅ Copied: $p → $DEST"; fi
              break
            fi
          done
          python - <<'PY'
          import os, pandas as pd, sys
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "neighbors.csv")
          try: df = pd.read_csv(p, dtype=str)
          except Exception: sys.exit(0)
          if df.empty: sys.exit(0)
          low = {c.lower(): c for c in df.columns}
          src = low.get("geoid") or low.get("src") or low.get("source")
          dst = low.get("neighbor") or low.get("neighbor_geoid") or low.get("neighborgeoid") or low.get("dst") or low.get("target")
          if not src or not dst: print("⚠️ neighbors.csv başlıkları tanınmadı:", df.columns.tolist()); sys.exit(0)
          df = df.rename(columns={src: "geoid", dst: "neighbor"})
          L = int(os.environ.get("GEOID_LEN","11"))
          for c in ("geoid","neighbor"):
              df[c] = df[c].astype(str).str.extract(r"(\d+)", expand=False).str.zfill(L)
          df = pd.concat([df, df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})], ignore_index=True)
          df = df.dropna().drop_duplicates(); df = df[df["geoid"] != df["neighbor"]]
          df[["geoid","neighbor"]].to_csv(p, index=False); print("Neighbors headers →", ["geoid","neighbor"], f"(rows={len(df)})")
          PY
          # kalite kontrol + gerekirse rebuild (script aynı kaldı)
          if [ -f "${CRIME_DATA_DIR}/.rebuild_neighbors" ]; then rm -f "${CRIME_DATA_DIR}/.rebuild_neighbors"; fi

      - name: 11) Hava durumu (SON ADIM, grid-level) → STACKING_DATASET’e ekle
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          STACKING_IN:  ${{ env.CRIME_DATA_DIR }}/sf_crime_events_enriched.csv
          STACKING_OUT: ${{ env.CRIME_DATA_DIR }}/sf_crime_events_enriched.csv
          WX_LOCATION:  ${{ env.WX_LOCATION }}
          WX_UNIT:      ${{ env.WX_UNIT }}
        run: |
          set -e
          if [ -f update_weather.py ]; then
            python -u update_weather.py --scope grid --join-keys date,hour --grid-in "${STACKING_IN}" --grid-out "${STACKING_OUT}" --location "${WX_LOCATION}" --unit "${WX_UNIT}"
          elif [ -f scripts/update_weather.py ]; then
            python -u scripts/update_weather.py --scope grid --join-keys date,hour --grid-in "${STACKING_IN}" --grid-out "${STACKING_OUT}" --location "${WX_LOCATION}" --unit "${WX_UNIT}"
          else
            echo "❌ Weather script not found"; exit 2
          fi
          echo "📄 Weather eklendi → ${STACKING_OUT} (ilk 5 satır)"; head -n 5 "${STACKING_OUT}" || true

      - name: 11.5) Olay-bazlı zenginleştirme (grid → sf_crime.csv merge)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DIR: ${{ env.CRIME_DATA_DIR }}
          GRID_PATH: ${{ env.CRIME_DATA_DIR }}/sf_crime_events_enriched.csv
          CRIME_PATH: ${{ env.CRIME_DATA_DIR }}/sf_crime.csv
          OUT_PATH: ${{ env.CRIME_DATA_DIR }}/sf_crime_events_enriched.csv
        run: |
          set -e
          python - <<'PY'
          import os, re, pandas as pd
          CRIME_DIR  = os.environ["CRIME_DIR"]
          GRID_PATH  = os.environ["GRID_PATH"]
          CRIME_PATH = os.environ["CRIME_PATH"]
          OUT_PATH   = os.environ["OUT_PATH"]

          # 1) Olay dosyası: date & hour üret
          df_e = pd.read_csv(CRIME_PATH, low_memory=False)
          cols = {c.lower(): c for c in df_e.columns}
          # Tarih/saat alanını bul
          ts_col = cols.get("timestamp") or cols.get("datetime") or cols.get("incident_datetime") or cols.get("date_time") or cols.get("date")
          if ts_col is None:
            raise RuntimeError("sf_crime.csv içinde timestamp/datetime sütunu bulunamadı.")
          t = pd.to_datetime(df_e[ts_col], errors="coerce")
          df_e["date"] = t.dt.strftime("%Y-%m-%d")
          df_e["hour"] = t.dt.hour

          # GEOID normalize (sadece rakam, soldan sıfır doldur)
          def norm_geoid(s, L=11):
              return (s.astype(str)
                        .str.extract(r"(\d+)", expand=False)
                        .fillna("")
                        .str.zfill(L))
          gcol_e = cols.get("geoid") or cols.get("geography_id") or "GEOID"
          df_e["GEOID"] = norm_geoid(df_e[gcol_e] if gcol_e in df_e.columns else df_e["GEOID"])

          # 2) Grid: aynı anahtarlar + sadece özellik kolonları
          df_g = pd.read_csv(GRID_PATH, low_memory=False)
          low_g = {c.lower(): c for c in df_g.columns}
          # Anahtar kolonlarını garanti et
          for need in ("geoid","date","hour"):
              if need not in low_g:
                  raise RuntimeError(f"Grid dosyasında '{need}' kolonu yok.")
          df_g.rename(columns={low_g["geoid"]:"GEOID", low_g["date"]:"date", low_g["hour"]:"hour"}, inplace=True)

          # Birden fazla satır/çakışma olursa anahtara göre son kaydı tut
          df_g = df_g.sort_index().drop_duplicates(subset=["GEOID","date","hour"], keep="last")

          # Sızma riskli kolonları at (etiket/çıktı/kümülatif hedef benzeri)
          drop_like = re.compile(r"^(y[_]?label|target|label|crime_count|prev_crime_|past_7d_crimes)$", re.I)
          feature_cols = [c for c in df_g.columns if c not in ("GEOID","date","hour") and not drop_like.match(c)]
          # İstersen burada whitelist de kullanabiliriz.

          # 3) Merge (left) → olay sayısı korunur (~550k)
          df_m = df_e.merge(df_g[["GEOID","date","hour"]+feature_cols],
                            on=["GEOID","date","hour"], how="left", copy=False)

          # 4) Kaydet
          df_m.to_csv(OUT_PATH, index=False)
          print("✅ Merge tamam:", OUT_PATH, "| rows:", len(df_m), "| cols:", len(df_m.columns))

          # 5) Opsiyonel: Parquet + _y snapshot
          try:
              import pyarrow  # noqa
              df_m.to_parquet(os.path.splitext(OUT_PATH)[0]+".parquet", index=False)
          except Exception as e:
              print("⚠️ Parquet yazılamadı:", e)
          import shutil
          shutil.copyfile(OUT_PATH, os.path.splitext(OUT_PATH)[0]+"_y.csv")
          PY

      - name: 12) Install ML deps for stacking
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          pip install pandas numpy scikit-learn joblib
          pip install xgboost lightgbm || true

      - name: 13) Run stacking risk pipeline
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_events_enriched.csv
          ENABLE_SPATIAL_TE: "1"
          TE_ALPHA: "50"
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          ENABLE_TE_ABLATION: "1"
          ABLASYON_BASIS: "ohe"
        run: |
          set -e
          if [ ! -f "${STACKING_DATASET}" ]; then echo "❌ STACKING_DATASET bulunamadı: ${STACKING_DATASET}"; exit 2; fi
          python -u stacking_risk_pipeline.py

      # --- FINAL: *_y.csv snapshot + Parquet + ZIP + Upload ---
      - name: 14) Snapshot (_y.csv) üret ve parquet’e çevir
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python - <<'PY'
          import os, re, glob, pandas as pd, pathlib
          root = os.environ.get("CRIME_DATA_DIR","crime_prediction_data_pre")
          pathlib.Path(root).mkdir(parents=True, exist_ok=True)
          csvs = [p for p in glob.glob(os.path.join(root,"*.csv")) if not p.endswith("_y.csv")]
          def y_name(p): 
              base = os.path.basename(p); stem, ext = os.path.splitext(base)
              return os.path.join(root, f"{stem}_y.csv")
          for p in csvs:
              try:
                  # y snapshot (günlük step – 1 günlük increment mantığıyla en güncel halin kopyası)
                  y = y_name(p)
                  if not os.path.exists(y):
                      open(y, "wb").write(open(p,"rb").read())
                  else:
                      # her çalıştırmada günceli y'ye kopyala (idempotent)
                      open(y, "wb").write(open(p,"rb").read())
                  # parquet
                  df = pd.read_csv(p, low_memory=False)
                  pq = os.path.join(root, os.path.splitext(os.path.basename(p))[0] + ".parquet")
                  df.to_parquet(pq, index=False)
                  print("✓", os.path.basename(p), "→", os.path.basename(y), "ve", os.path.basename(pq))
              except Exception as e:
                  print("⚠️ Parquet/snapshot atlandı:", p, e)
          PY

      - name: 15) Zip sonuçlar (crime_prediction_data_pre.zip)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          rm -f "${ARTIFACT_ZIP}"
          zip -r "${ARTIFACT_ZIP}" "${CRIME_DATA_DIR}" -x "*.git*" || true
          echo "📦 Oluşturuldu → ${ARTIFACT_ZIP}"

      - name: 16) Upload artifact (cem5113/crime_prediction_data_pre tarzı)
        if: ${{ steps.gate.outputs.proceed == 'true' && github.event.inputs.persist != 'none' }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.ARTIFACT_NAME }}
          path: |
            ${{ env.ARTIFACT_ZIP }}
            ${{ env.CRIME_DATA_DIR }}/**/*.parquet
            ${{ env.CRIME_DATA_DIR }}/*_y.csv
          compression-level: 0
          if-no-files-found: warn 
