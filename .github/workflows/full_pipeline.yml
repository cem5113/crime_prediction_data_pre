name: Full SF Crime Pipeline

on:
  schedule:
    - cron: "0 14,15 * * *"   # SF 07:00: 14:00 UTC (yaz), 15:00 UTC (kƒ±≈ü)
  workflow_dispatch:
    inputs:
      persist:
        description: "Sonu√ßlarƒ± nasƒ±l saklayalƒ±m?"
        type: choice
        options: [artifact, commit, none]
        default: artifact
      force:
        description: "Manuel tetiklemede 07:00 kapƒ±sƒ±nƒ± YOK SAY"
        type: boolean
        default: true
      top_k:
        description: "Stacking: her saat dilimi i√ßin √∂nerilecek GEOID sayƒ±sƒ±"
        default: "50"

permissions:
  actions: read
  contents: write

concurrency:
  group: full-pipeline-${{ github.ref }}
  cancel-in-progress: true

env:
  CRIME_DATA_DIR: crime_prediction_data
  GEOID_LEN: "11"

  BACKFILL_DAYS: "0"
  SF_SODA_FETCH_STRATEGY: ${{ vars.SF_SODA_FETCH_STRATEGY || 'bulk' }}
  SF_SODA_PAGE_LIMIT:    ${{ vars.SF_SODA_PAGE_LIMIT    || '50000' }}
  SF_SODA_MAX_PAGES:     ${{ vars.SF_SODA_MAX_PAGES     || '100' }}

  SF911_API_URL:       ${{ vars.SF911_API_URL       || 'https://data.sfgov.org/resource/2zdj-bwza.json' }}
  SF911_AGENCY_FILTER: ${{ vars.SF911_AGENCY_FILTER || 'agency like "%Police%"' }}
  SF911_API_TOKEN:     ${{ secrets.SF911_API_TOKEN }}
  SOCS_APP_TOKEN:      ${{ secrets.SOCS_APP_TOKEN }}

  PATROL_TOP_K: ${{ github.event.inputs.top_k || '50' }}

  PATROL_HORIZON_DAYS: "3"
  WX_LOCATION: "san francisco"
  WX_UNIT: "us"

  ACS_YEAR: ${{ vars.ACS_YEAR || 'LATEST' }}
  DEMOG_WHITELIST: ${{ vars.DEMOG_WHITELIST || '' }}
  CENSUS_GEO_LEVEL: ${{ vars.CENSUS_GEO_LEVEL || 'auto' }}

jobs:
  run:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout (with LFS)
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Ensure LFS objects
        run: |
          git lfs install
          git lfs pull
          echo "Repo root:"; ls -lah
          echo "crime_prediction_data:"; ls -lah crime_prediction_data || true
          echo "CRIME_DATA_DIR:"; ls -lah "${CRIME_DATA_DIR}" || true

      - name: Ensure data dir
        run: mkdir -p "${CRIME_DATA_DIR}"

      # --- SF 07:00 kapƒ±sƒ± (TZ ayarla + saat kontrol√º) ---
      - name: Set runner timezone to America/Los_Angeles
        uses: szenius/set-timezone@v2.0
        with:
          timezoneLinux: "America/Los_Angeles"

      - name: Gate by local SF time == 07 (bypassable)
        id: gate
        run: |
          now="$(date)"
          echo "Runner local time: $now"
          echo "RUN_LOCAL_TIME=$now" >> $GITHUB_ENV
          # Manuel + force=true ise doƒürudan ge√ß
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.force }}" = "true" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          # Aksi halde 07:00 kontrol√º
          if [ "$(date +%H)" = "07" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
          else
            echo "proceed=false" >> $GITHUB_OUTPUT
          fi

      - name: Skip summary (outside 07:00 and not forced)
        if: ${{ steps.gate.outputs.proceed != 'true' }}
        run: |
          {
            echo "## SF Crime Pipeline"
            echo ""
            echo "- √áalƒ±≈üma zamanƒ± (SF): **$(date)**"
            echo "- Not: 07:00 kapƒ±sƒ± nedeniyle adƒ±mlar atlandƒ±. Manuel tetiklerken \`force=true\` verin."
          } >> $GITHUB_STEP_SUMMARY

      - name: System deps for rtree (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: sudo apt-get update && sudo apt-get install -y libspatialindex-dev

      - name: Set up Python 3.11
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install -U "geopandas==1.0.1" "shapely==2.0.4" "pyproj==3.6.1" "pyogrio==0.9.0" "rtree==1.3.0"

      - name: Geo stack smoke test
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import geopandas, shapely, pyproj, pyogrio, pandas
          print("geopandas", geopandas.__version__)
          print("shapely", shapely.__version__)
          print("pyproj", pyproj.__version__)
          print("pyogrio", pyogrio.__version__)
          print("pandas", pandas.__version__)
          PY

      # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
      # Prefetch sf_crime_y.csv (GH_TOKEN set ‚Üí login YOK)
      - name: 00) Prefetch sf_crime_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}              # gh CLI bunu otomatik kullanƒ±r; login YOK
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_crime_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""

          # Son 10 ba≈üarƒ±lƒ± run i√ßinde ara; ilk bulduƒüunu kopyala
          RUNS=$(gh run list -R "${GITHUB_REPOSITORY}" \
                  --workflow "${WORKFLOW_NAME}" --status success \
                  -L 10 --json databaseId -q '.[].databaseId' || true)

          if [ -z "${RUNS}" ]; then
            echo "‚ÑπÔ∏è Ba≈üarƒ±lƒ± ge√ßmi≈ü run bulunamadƒ±; release fallback devreye girecek."
            exit 0
          fi

          for RID in ${RUNS}; do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; update_crime.py release fallback kullanacak."
          fi
      # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

      # ---- PIPELINE ----
      - name: 01) Su√ß tabanƒ± ve grid ‚Üí sf_crime.csv + gridler
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_crime.py

      - name: Restore 911 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
          key: 911-${{ runner.os }}-${{ github.run_number }}
          restore-keys: |
            911-${{ runner.os }}-

      # ‚úÖ GH auth: login YOK (GH_TOKEN var)
      - name: Prefetch sf_911_last_5_year_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}   # gh i√ßin non-interactive auth
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_911_last_5_year_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" \
                        --workflow "${WORKFLOW_NAME}" --status success \
                        -L 10 --json databaseId -q '.[].databaseId'); do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; script release fallback kullanacak."
          fi

      # 911: g√ºn g√ºn yerine bulk (limit=50k)
      - name: 02) 911 ‚Üí sf_crime_01.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SF911_API_URL:         ${{ env.SF911_API_URL }}
          SF911_AGENCY_FILTER:   ${{ env.SF911_AGENCY_FILTER }}
          SF911_API_TOKEN:       ${{ env.SF911_API_TOKEN }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
        run: |
          set -e
          python -u update_911.py

      - name: Restore 311 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv
          key: 311-${{ runner.os }}-${{ github.run_number }}
          restore-keys: |
            311-${{ runner.os }}-

      # ‚úÖ YENƒ∞: 311 √∂zet dosyasƒ± i√ßin artifact prefetch (GH login YOK)
      - name: Prefetch sf_311_last_5_years_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_311_last_5_years_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" \
                        --workflow "${WORKFLOW_NAME}" --status success \
                        -L 10 --json databaseId -q '.[].databaseId'); do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; update_311.py kendi akƒ±≈üƒ±yla devam edecek."
          fi

      # 311: aynƒ± ≈üekilde bulk √ßekim
      - name: 03) 311 ‚Üí sf_crime_02.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
        run: |
          set -e
          if [ -f update_311.py ]; then
            python -u update_311.py
          else
            python -u scripts/update_311.py
          fi

      - name: Ensure population CSV (local file)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          mkdir -p "${CRIME_DATA_DIR}"
          DEST="${CRIME_DATA_DIR}/sf_population.csv"
          CANDIDATES=(
            "${CRIME_DATA_DIR}/sf_population.csv"
            "sf_population.csv"
            "data/sf_population.csv"
            "inputs/sf_population.csv"
          )
          FOUND=""
          for p in "${CANDIDATES[@]}"; do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then
                echo "‚ÑπÔ∏è Zaten hedefte: $DEST"
              else
                cp -f "$p" "$DEST"
                echo "‚úÖ Copied: $p ‚Üí $DEST"
              fi
              FOUND="yes"
              break
            fi
          done
          if [ -z "$FOUND" ]; then
            echo "‚ùå sf_population.csv bulunamadƒ±. L√ºtfen repo‚Äôya ekleyin veya ${CRIME_DATA_DIR} altƒ±na yerle≈ütirin."
            exit 2
          fi
          echo "---- sf_population.csv (head) ----"
          head -n 5 "$DEST" || true

      - name: (Opsiyonel) sf_population.csv ba≈ülƒ±k normalize
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import os, pandas as pd
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "sf_population.csv")
          df = pd.read_csv(p)
          low = {c.lower(): c for c in df.columns}

          # GEOID standardizasyonu
          if 'geoid' not in low and 'geography_id' in low:
              df.rename(columns={low['geography_id']: 'GEOID'}, inplace=True)

          # N√ºfus s√ºtununu 'population' yap
          for cand in ['population','total_population','b01003_001e','estimate','total','value']:
              if cand in low:
                  if 'population' not in df.columns:
                      df.rename(columns={low[cand]: 'population'}, inplace=True)
                  break

          df.to_csv(p, index=False)
          print("Normalized headers:", df.columns.tolist())
          PY

      - name: 04) N√ºfus ‚Üí sf_crime_03.csv (demografi + n√ºfus)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          POPULATION_PATH:  ${{ env.CRIME_DATA_DIR }}/sf_population.csv
          CENSUS_GEO_LEVEL: ${{ env.CENSUS_GEO_LEVEL }}
          ACS_YEAR:         ${{ env.ACS_YEAR }}
          DEMOG_WHITELIST:  ${{ env.DEMOG_WHITELIST }}
        run: |
          set -e
          echo "---- update_population.py (ilk 30 satƒ±r) ----"
          sed -n '1,30p' update_population.py || true
          python -u update_population.py

      - name: 05) Otob√ºs ‚Üí sf_crime_04.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_bus.py

      - name: 06) Tren (BART) ‚Üí sf_crime_05.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_train.py

      - name: 07) POI zenginle≈ütirme ‚Üí sf_crime_06.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_poi.py ]; then python -u update_poi.py;
          elif [ -f pipeline_make_sf_crime_06.py ]; then python -u pipeline_make_sf_crime_06.py;
          else echo "POI adƒ±mƒ± bulunamadƒ±"; exit 2; fi

      - name: 08) Polis & Devlet binalarƒ± ‚Üí sf_crime_07.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_police_gov.py ]; then python -u update_police_gov.py;
          elif [ -f scripts/enrich_police_gov_06_to_07.py ]; then python -u scripts/enrich_police_gov_06_to_07.py;
          else echo "Polis/Gov adƒ±mƒ± bulunamadƒ±"; exit 2; fi

      - name: 09) Hava durumu ‚Üí sf_crime_08.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_weather.py ]; then
            python -u update_weather.py
          elif [ -f scripts/update_weather.py ]; then
            python -u scripts/update_weather.py
          else
            echo "‚ùå Weather script not found"; exit 2
          fi
          echo "sf_crime_08.csv ‚Äî ilk 5 satƒ±r:"
          head -n 5 "${CRIME_DATA_DIR}/sf_crime_08.csv" || head -n 5 sf_crime_08.csv || true
            
      - name: 10) sf_crime_08_static √ºretimi (07 ‚Üí 08_static)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python - <<'PY'
          import os
          import pandas as pd
          from pathlib import Path
      
          CRIME_DIR = os.getenv("CRIME_DATA_DIR","crime_prediction_data")
          candidates = [
              Path(CRIME_DIR) / "sf_crime_07.csv",
              Path("crime_prediction_data") / "sf_crime_07.csv",
              Path("crime_data") / "sf_crime_07.csv",
              Path("sf_crime_07.csv"),
              Path("outputs") / "sf_crime_07.csv",
          ]
          src_path = next((p for p in candidates if p.exists()), None)
          if src_path is None:
              raise FileNotFoundError("sf_crime_07.csv not found in known locations")
      
          dst = str(Path(CRIME_DIR)/"sf_crime_08_static.csv")
      
          try:
              from scripts.common import clean_and_save_crime_08
              clean_and_save_crime_08(str(src_path), dst)
              print("‚úÖ sf_crime_08_static hazƒ±r ‚Üí", dst)
          except Exception as e:
              # Fonksiyon yoksa ya da hata verdiyse, minimum bir fallback uygula (passthrough)
              print(f"‚ö†Ô∏è clean_and_save_crime_08 kullanƒ±lamadƒ± ({e}); passthrough yazƒ±lƒ±yor.")
              df = pd.read_csv(src_path, low_memory=False)
              df.to_csv(dst, index=False)
              print("‚úÖ Passthrough 08_static yazƒ±ldƒ± ‚Üí", dst)
      
          try:
              df = pd.read_csv(dst, low_memory=False)
              with pd.option_context("display.max_columns", None, "display.width", 2000):
                  print("üìÑ sf_crime_08_static.csv ‚Äî ilk 5 satƒ±r:")
                  print(df.head(5).to_string(index=False))
          except Exception as e:
              print(f"‚ö†Ô∏è √ñnizleme okunamadƒ±: {e}")
          PY

      - name: 10.1) (Opsiyonel) Eski sf_crime_09'dan ekstra statikleri 08_static'e kat
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import os, pandas as pd
          from pathlib import Path
          CRIME_DIR = os.environ.get("CRIME_DATA_DIR","crime_prediction_data")
          old09 = Path(CRIME_DIR)/"sf_crime_09.csv"
          out08 = Path(CRIME_DIR)/"sf_crime_08_static.csv"
          if not (old09.exists() and out08.exists()):
              print("‚ÑπÔ∏è eski 09 veya 08_static yok ‚Üí atlandƒ±"); raise SystemExit(0)
          df09 = pd.read_csv(old09, low_memory=False)
          drop_names = {
            "temp","tempmax","temp_min","tempmin","temp_max","temp_range",
            "precip","precipitation","precipitation_mm","humidity","windspeed","icon","description"
          }
          df09 = df09[[c for c in df09.columns if c.lower() not in drop_names]]
          if "GEOID" in df09.columns:
              df09 = df09.drop_duplicates(subset=["GEOID"], keep="last")
              base = pd.read_csv(out08, low_memory=False)
              cols_new = [c for c in df09.columns if c not in base.columns and c != "GEOID"]
              merged = base.merge(df09[["GEOID"]+cols_new], on="GEOID", how="left") if cols_new else base
              merged.to_csv(out08, index=False)
              print("‚ûï eski 09'dan 08_static'e eklenen kolonlar:", cols_new)
          PY

      - name: 10.5) neighbors.csv (kullan/normalize/√ºret + kalite kontrol)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          NEIGHBOR_STRATEGY: queen         # queen|rook
          MIN_NEIGHBOR_DEG: "3"            # en az derece
          KNN_K: "5"                       # takviye i√ßin k
          NEIGHBOR_POLY: ${{ env.CRIME_DATA_DIR }}/sf_grid.geojson
        run: |
          set -euo pipefail
          DEST="${CRIME_DATA_DIR}/neighbors.csv"
          mkdir -p "${CRIME_DATA_DIR}"

          echo "‚Üí hedef: $DEST"
          # 1) Varsa repodan kopyala
          for p in \
            "${CRIME_DATA_DIR}/neighbors.csv" \
            "crime_prediction_data/neighbors.csv" \
            "neighbors.csv" "data/neighbors.csv" "inputs/neighbors.csv"
          do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then
                echo "‚ÑπÔ∏è Zaten hedefte: $DEST"
              else
                cp -f "$p" "$DEST"
                echo "‚úÖ Copied: $p ‚Üí $DEST"
              fi
              break
            fi
          done

          # 2) Ba≈ülƒ±klarƒ± normalize et ‚Üí geoid,neighbor
          python - <<'PY'
          import os, pandas as pd, sys
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "neighbors.csv")
          try:
              df = pd.read_csv(p, dtype=str)
          except Exception:
              sys.exit(0)
          if df.empty:
              sys.exit(0)
          low = {c.lower(): c for c in df.columns}
          src = low.get("geoid") or low.get("src") or low.get("source")
          dst = (low.get("neighbor") or low.get("neighbor_geoid") or
                 low.get("neighborgeoid") or low.get("dst") or low.get("target"))
          if not src or not dst:
              print("‚ö†Ô∏è neighbors.csv ba≈ülƒ±klarƒ± tanƒ±nmadƒ±:", df.columns.tolist()); sys.exit(0)
          df = df.rename(columns={src: "geoid", dst: "neighbor"})
          L = int(os.environ.get("GEOID_LEN","11"))
          for c in ("geoid","neighbor"):
              df[c] = df[c].astype(str).str.extract(r"(\d+)", expand=False).str.zfill(L)
          # √ßift y√∂nl√º ve tekille
          df = pd.concat([df, df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})], ignore_index=True)
          df = df.dropna().drop_duplicates()
          df = df[df["geoid"] != df["neighbor"]]
          df[["geoid","neighbor"]].to_csv(p, index=False)
          print("Neighbors headers ‚Üí", ["geoid","neighbor"], f"(rows={len(df)})")
          PY

          # 3) Kalite kontrol
          python - <<'PY'
          import os, pandas as pd
          from pathlib import Path
          p = Path(os.environ["CRIME_DATA_DIR"]) / "neighbors.csv"
          min_deg = int(os.environ.get("MIN_NEIGHBOR_DEG","3"))
          if not p.exists() or p.stat().st_size == 0:
              need=True; reason="yok/bo≈ü"
          else:
              try:
                  df = pd.read_csv(p, dtype=str)
                  ok = set(df.columns) >= {"geoid","neighbor"} and len(df)>0
                  mindeg = (df.groupby("geoid")["neighbor"].nunique().min() if ok else 0)
                  need = (not ok) or (mindeg is None) or (mindeg < min_deg)
                  reason = f"min_deg={mindeg}<{min_deg}" if ok else "ge√ßersiz ba≈ülƒ±k/bo≈ü"
              except Exception as e:
                  need=True; reason=f"hata:{e}"
          if need:
              Path(os.environ["CRIME_DATA_DIR"]).joinpath(".rebuild_neighbors").write_text(reason)
              print("REBUILD=1 ‚Üí", reason)
          else:
              print("REBUILD=0 ‚Üí yeterli kalite")
          PY

      - name: "10.5) neighbors.csv (kullan/normalize/√ºret + kalite kontrol)"
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          NEIGHBOR_STRATEGY: queen
          MIN_NEIGHBOR_DEG: "3"
          KNN_K: "5"
          NEIGHBOR_POLY: ${{ env.CRIME_DATA_DIR }}/sf_grid.geojson
        run: |
          set -euo pipefail
          DEST="${CRIME_DATA_DIR}/neighbors.csv"
          mkdir -p "${CRIME_DATA_DIR}"

          echo "‚Üí hedef: $DEST"
          # 1) Varsa repodan kopyala
          for p in \
            "${CRIME_DATA_DIR}/neighbors.csv" \
            "crime_prediction_data/neighbors.csv" \
            "neighbors.csv" "data/neighbors.csv" "inputs/neighbors.csv"
          do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then
                echo "‚ÑπÔ∏è Zaten hedefte: $DEST"
              else
                cp -f "$p" "$DEST"
                echo "‚úÖ Copied: $p ‚Üí $DEST"
              fi
              break
            fi
          done

          # 2) Ba≈ülƒ±k normalize ‚Üí geoid,neighbor
          python - <<'PY'
          import os, pandas as pd, sys
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "neighbors.csv")
          try:
              df = pd.read_csv(p, dtype=str)
          except Exception:
              sys.exit(0)
          if df.empty:
              sys.exit(0)
          low = {c.lower(): c for c in df.columns}
          src = low.get("geoid") or low.get("src") or low.get("source")
          dst = (low.get("neighbor") or low.get("neighbor_geoid") or
                 low.get("neighborgeoid") or low.get("dst") or low.get("target"))
          if not src or not dst:
              print("‚ö†Ô∏è neighbors.csv ba≈ülƒ±klarƒ± tanƒ±nmadƒ±:", df.columns.tolist()); sys.exit(0)
          df = df.rename(columns={src: "geoid", dst: "neighbor"})
          L = int(os.environ.get("GEOID_LEN","11"))
          for c in ("geoid","neighbor"):
              df[c] = df[c].astype(str).str.extract(r"(\d+)", expand=False).str.zfill(L)
          df = pd.concat([df, df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})], ignore_index=True)
          df = df.dropna().drop_duplicates()
          df = df[df["geoid"] != df["neighbor"]]
          df[["geoid","neighbor"]].to_csv(p, index=False)
          print("Neighbors headers ‚Üí", ["geoid","neighbor"], f"(rows={len(df)})")
          PY

          # 3) Kalite kontrol
          python - <<'PY'
          import os, pandas as pd
          from pathlib import Path
          p = Path(os.environ["CRIME_DATA_DIR"]) / "neighbors.csv"
          min_deg = int(os.environ.get("MIN_NEIGHBOR_DEG","3"))
          if not p.exists() or p.stat().st_size == 0:
              need=True; reason="yok/bo≈ü"
          else:
              try:
                  df = pd.read_csv(p, dtype=str)
                  ok = set(df.columns) >= {"geoid","neighbor"} and len(df)>0
                  mindeg = (df.groupby("geoid")["neighbor"].nunique().min() if ok else 0)
                  need = (not ok) or (mindeg is None) or (mindeg < min_deg)
                  reason = f"min_deg={mindeg}<{min_deg}" if ok else "ge√ßersiz ba≈ülƒ±k/bo≈ü"
              except Exception as e:
                  need=True; reason=f"hata:{e}"
          if need:
              Path(os.environ["CRIME_DATA_DIR"]).joinpath(".rebuild_neighbors").write_text(reason)
              print("REBUILD=1 ‚Üí", reason)
          else:
              print("REBUILD=0 ‚Üí yeterli kalite")
          PY

          # 4) Yeniden √ºretim gerekiyorsa
          if [ -f "${CRIME_DATA_DIR}/.rebuild_neighbors" ]; then
            if [ ! -f "${NEIGHBOR_POLY}" ]; then
              echo "‚ö†Ô∏è Polygon katmanƒ± yok: ${NEIGHBOR_POLY}. Mevcut neighbors.csv kullanƒ±lacak."
            else
              echo "üîß Kom≈üuluk yeniden olu≈üturuluyor (${NEIGHBOR_STRATEGY}, min_deg=${MIN_NEIGHBOR_DEG}, knn_k=${KNN_K})"
              cat <<-'PY' > _rebuild_neighbors.py
              #!/usr/bin/env python3
              from __future__ import annotations
              import os, re
              from pathlib import Path
              import pandas as pd
              try:
                  import geopandas as gpd
              except Exception:
                  gpd = None

              CRIME_DIR  = Path(os.environ.get("CRIME_DATA_DIR","crime_prediction_data"))
              GEOID_LEN  = int(os.environ.get("GEOID_LEN","11"))
              STRATEGY   = os.environ.get("NEIGHBOR_STRATEGY","queen").lower()
              MIN_DEG    = int(os.environ.get("MIN_NEIGHBOR_DEG","3"))
              KNN_K      = int(os.environ.get("KNN_K","5"))
              OUT_PATH   = Path(os.environ.get("NEIGHBOR_FILE", str(CRIME_DIR/"neighbors.csv")))
              GRID_CSV   = Path(os.environ.get("STACKING_DATASET", str(CRIME_DIR/"sf_crime_grid_full_labeled.csv")))
              POLY_HINT  = os.environ.get("NEIGHBOR_POLY","")

              def _norm(s: pd.Series) -> pd.Series:
                  return s.astype(str).str.extract(r"(\d+)", expand=False).str.zfill(GEOID_LEN)

              def _sym(df: pd.DataFrame) -> pd.DataFrame:
                  rev = df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})
                  out = pd.concat([df, rev], ignore_index=True)
                  out = out[out["geoid"] != out["neighbor"]]
                  return out.drop_duplicates()

              def _neighbors_from_polygons(path: str) -> pd.DataFrame | None:
                  if not gpd:
                      return None
                  try:
                      gdf = gpd.read_file(path)
                  except Exception:
                      return None
                  low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in gdf.columns}
                  gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid") or list(gdf.columns)[0]
                  if "geometry" not in gdf.columns:
                      return None
                  gdf = gdf[[gcol,"geometry"]].rename(columns={gcol:"geoid"}).copy()
                  gdf["geoid"] = _norm(gdf["geoid"])
                  gdf = gdf[gdf["geometry"].notna()].reset_index(drop=True)
                  pairs = gdf.sindex.query_bulk(gdf.geometry, predicate="touches")
                  i, j = pairs
                  m = pd.DataFrame({"i": i, "j": j})
                  m = m[m["i"] < m["j"]]
                  df = pd.DataFrame({
                      "geoid":    gdf.loc[m["i"], "geoid"].to_numpy(),
                      "neighbor": gdf.loc[m["j"], "geoid"].to_numpy(),
                  })
                  return _sym(df.drop_duplicates())

              def _centroids_table(poly_ok: bool) -> pd.DataFrame | None:
                  if poly_ok and gpd:
                      gdf = gpd.read_file(POLY_HINT)
                      low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in gdf.columns}
                      gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid") or list(gdf.columns)[0]
                      c = gdf[[gcol,"geometry"]].rename(columns={gcol:"geoid"}).copy()
                      c["geoid"] = _norm(c["geoid"])
                      cent = c.geometry.centroid
                      return pd.DataFrame({"geoid": c["geoid"], "x": cent.x, "y": cent.y})
                  if GRID_CSV.exists():
                      df = pd.read_csv(GRID_CSV, dtype=str, low_memory=False)
                      low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in df.columns}
                      gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid")
                      lat  = low.get("lat") or low.get("latitude") or low.get("centroidlat")
                      lon  = low.get("lon") or low.get("lng") or low.get("longitude") or low.get("centroidlon")
                      if gcol and lat and lon:
                          t = df[[gcol, lat, lon]].rename(columns={gcol:"geoid", lat:"y", lon:"x"}).copy()
                          t["geoid"] = _norm(t["geoid"])
                          t["x"] = pd.to_numeric(t["x"], errors="coerce")
                          t["y"] = pd.to_numeric(t["y"], errors="coerce")
                          return t.dropna()
                  return None

              def _ensure_min_degree(nb: pd.DataFrame, geos: pd.DataFrame) -> pd.DataFrame:
                  import numpy as np
                  if geos is None or geos.empty:
                      return nb
                  deg = nb.groupby("geoid")["neighbor"].nunique()
                  need = set(deg.index[deg < MIN_DEG] if not deg.empty else geos["geoid"])
                  if not need:
                      return nb
                  idx = {g:i for i,g in enumerate(geos["geoid"])}
                  XY  = geos[["x","y"]].to_numpy()
                  add = []
                  for g in need:
                      i = idx[g]
                      d = np.sum((XY - XY[i])**2, axis=1)
                      order = np.argsort(d)
                      picks = []
                      for j in order:
                          gj = geos.iloc[j]["geoid"]
                          if gj == g:
                              continue
                          picks.append(gj)
                          if len(picks) >= KNN_K:
                              break
                      add += [(g, n) for n in picks]
                  nb2 = pd.concat([nb, pd.DataFrame(add, columns=["geoid","neighbor"])], ignore_index=True)
                  return _sym(nb2.drop_duplicates())

              def main():
                  poly_ok = bool(POLY_HINT) and Path(POLY_HINT).exists()
                  nb = _neighbors_from_polygons(POLY_HINT) if poly_ok else pd.DataFrame(columns=["geoid","neighbor"])
                  cent = _centroids_table(poly_ok)
                  nb = _ensure_min_degree(nb, cent)
                  if nb.empty:
                      nb = pd.DataFrame(columns=["geoid","neighbor"])
                  nb["geoid"] = _norm(nb["geoid"])
                  nb["neighbor"] = _norm(nb["neighbor"])
                  nb = _sym(nb).drop_duplicates().sort_values(["geoid","neighbor"])
                  OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
                  nb.to_csv(OUT_PATH, index=False)
                  deg = nb.groupby("geoid")["neighbor"].nunique()
                  print(f"neighbors.csv yazƒ±ldƒ± ‚Üí {OUT_PATH} | n_edges={len(nb)} | n_nodes={len(deg)} | min_deg={int(deg.min()) if len(deg) else 0} | mean_deg={round(float(deg.mean()),2) if len(deg) else 0.0}")

              if __name__ == "__main__":
                  main()
              PY

              python _rebuild_neighbors.py
              rm -f _rebuild_neighbors.py
              rm -f "${CRIME_DATA_DIR}/.rebuild_neighbors"
            fi
          fi

          echo "neighbors.csv (ilk 10 satƒ±r):"
          head -n 10 "${DEST}" || true

      - name: "11) Install ML deps for stacking"
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          pip install pandas numpy scikit-learn joblib
          pip install xgboost lightgbm || true

      - name: "12) Run stacking risk pipeline"
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          ENABLE_SPATIAL_TE: "1"
          TE_ALPHA: "50"
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          ENABLE_TE_ABLATION: "1"
          ABLASYON_BASIS: "ohe"
        run: |
          set -e
          if [ ! -f "${STACKING_DATASET}" ]; then
            echo "‚ùå STACKING_DATASET bulunamadƒ±: ${STACKING_DATASET}"
            exit 2
          fi
          python -u stacking_risk_pipeline.py

