name: Full SF Crime Pipeline (CSV → _y.csv priority + Parquet artifact)

on:
  schedule:
    - cron: "0 14,15 * * *"
  workflow_dispatch:
    inputs:
      persist:
        description: "Sonuçları nasıl saklayalım?"
        type: choice
        options: [artifact, commit, none]
        default: artifact
      force:
        description: "07:00 kapısını yok say"
        type: boolean
        default: true
      top_k:
        description: "Stacking: saat başı kaç GEOID?"
        default: "50"

permissions:
  actions: read
  contents: write

concurrency:
  group: full-pipeline-${{ github.ref }}
  cancel-in-progress: true

env:
  RIME_DATA_DIR: crime_prediction_data_pre 
  GEOID_LEN: "10"
  USE_PARQUET: "1"
  Y_PARQUET_COMPRESSION: "zstd"
  ID_BASED: "1"
  LABEL_STYLE: "legacy01"
  HORIZONS: "24h,7d,30d,365d"

  BACKFILL_DAYS: "0"            # Artifact/_y bulunmazsa 182’ye yükseltilecek
  SF_SODA_FETCH_STRATEGY: ${{ vars.SF_SODA_FETCH_STRATEGY || 'bulk' }}
  SF_SODA_PAGE_LIMIT:    ${{ vars.SF_SODA_PAGE_LIMIT    || '50000' }}
  SF_SODA_MAX_PAGES:     ${{ vars.SF_SODA_MAX_PAGES     || '100' }}

  SF911_API_URL:       ${{ vars.SF911_API_URL       || 'https://data.sfgov.org/resource/2zdj-bwza.json' }}
  SF911_AGENCY_FILTER: ${{ vars.SF911_AGENCY_FILTER || 'agency like "%Police%"' }}
  SF911_API_TOKEN:     ${{ secrets.SF911_API_TOKEN }}
  SOCS_APP_TOKEN:      ${{ secrets.SOCS_APP_TOKEN }}

  PATROL_TOP_K: "50"
  PATROL_HORIZON_DAYS: "3"
  WX_LOCATION: "san francisco"
  WX_UNIT: "us"

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout (with LFS)
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Ensure LFS objects
        run: |
          git lfs install
          git lfs pull
          echo "Repo root:"; ls -lah
          echo "crime_prediction_data_pre:"; ls -lah crime_prediction_data_pre || true

      - name: Ensure data dir
        run: mkdir -p "${CRIME_DATA_DIR}"

      - name: Symlink for legacy paths
        run: ln -sfn "$CRIME_DATA_DIR" crime_prediction_data

      # --- Previous artifact to reuse _y/parquet inputs ---
      - name: Install jq & fetch latest prior artifact (if exists)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          sudo apt-get update && sudo apt-get install -y jq curl unzip
          REPO="${{ github.repository }}"
          url="https://api.github.com/repos/${REPO}/actions/artifacts?per_page=100"
          curl -sS -H "Authorization: Bearer $GH_TOKEN" "$url" > artifacts.json
          ART_URL=$(jq -r '.artifacts | map(select(.name=="sf-crime-pipeline-output" and .expired==false)) | sort_by(.created_at) | last | .archive_download_url // ""' artifacts.json)
          if [ -n "$ART_URL" ]; then
            mkdir -p artifact_prev && cd artifact_prev
            curl -L -H "Authorization: Bearer $GH_TOKEN" -o prev.zip "$ART_URL"
            unzip -q prev.zip || true
            cd ..
            echo "FROM_ARTIFACT=1" >> $GITHUB_ENV
          else
            echo "FROM_ARTIFACT=0" >> $GITHUB_ENV
          fi

      - name: Decide BACKFILL (first run fallback)
        run: |
          if [ "${FROM_ARTIFACT}" = "0" ]; then
            echo "BACKFILL_DAYS=182" >> $GITHUB_ENV
          fi

      - name: Set runner timezone to America/Los_Angeles
        uses: szenius/set-timezone@v2.0
        with:
          timezoneLinux: "America/Los_Angeles"

      - name: Gate by local SF time == 07 (bypassable)
        id: gate
        run: |
          now="$(date)"
          echo "RUN_LOCAL_TIME=$now" >> $GITHUB_ENV
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.force }}" = "true" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT; exit 0
          fi
          if [ "$(date +%H)" = "07" ]; then echo "proceed=true" >> $GITHUB_OUTPUT; else echo "proceed=false" >> $GITHUB_OUTPUT; fi

      - name: Stage prior artifact files into CRIME_DATA_DIR
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          if [ "${FROM_ARTIFACT}" = "1" ]; then
            rsync -av artifact_prev/ "${CRIME_DATA_DIR}/" || true
          fi
          ls -lah "${CRIME_DATA_DIR}" || true

      - name: Set up Python 3.11
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install -U pandas numpy pyarrow polars scikit-learn joblib xgboost lightgbm geopandas shapely pyproj pyogrio rtree

      # ---------- Helpers ----------
      - name: Helper — pick_input & prefer *_y
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        shell: bash
        run: |
          cat > pick_input.sh <<'BASH'
          #!/usr/bin/env bash
          pick_input() {
            local out="$1"; shift
            local p
            for p in "$@"; do
              if [ -f "$p" ]; then
                printf -v "$out" "%s" "$p"; export "$out"; return 0
              fi
            done
            return 1
          }
          BASH
          chmod +x pick_input.sh

      # =================== PIPELINE ===================

      # 1) sf_crime.csv → sf_crime_y.csv (daily upsert; fallback backfill) + Y label rule (>=2) → sf_crime_01.csv
      - name: 01) Crime base → sf_crime_y.csv + Y (>=2) → sf_crime_01.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          GEOID_LEN: ${{ env.GEOID_LEN }}
          Y_PARQUET_COMPRESSION: ${{ env.Y_PARQUET_COMPRESSION }}
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, pandas as pd, numpy as np
          from datetime import timezone
          import pyarrow as pa, pyarrow.parquet as pq

          d=os.environ["CRIME_DATA_DIR"]
          y_csv=os.path.join(d,"sf_crime_y.csv")
          base_csv_candidates=[y_csv, os.path.join(d,"sf_crime.csv"), "sf_crime.csv", "data/sf_crime.csv"]
          src=None
          for p in base_csv_candidates:
            if os.path.exists(p):
              src=p; break
          if src is None: raise SystemExit("sf_crime(.csv) not found")

          # read
          df=pd.read_csv(src, low_memory=False)
          # ensure datetime col name (guess common columns)
          tcol=None
          for c in ["incident_datetime","datetime","occurred_at","timestamp","date"]:
            if c in df.columns: tcol=c; break
          if tcol is None: raise SystemExit("no datetime column")

          gcol=None
          for c in ["GEOID","geoid","GEOID10","geography_id"]:
            if c in df.columns: gcol=c; break
          if gcol is None: raise SystemExit("no GEOID column")

          s=pd.to_datetime(df[tcol], errors="coerce", utc=True).dt.tz_convert("America/Los_Angeles")
          df["_day"]=s.dt.day
          df["_month"]=s.dt.month
          m=df["_month"]
          # seasons (DJF, MAM, JJA, SON) → {winter=12,1,2; spring=3,4,5; summer=6,7,8; fall=9,10,11}
          season=np.select([m.isin([12,1,2]), m.isin([3,4,5]), m.isin([6,7,8]), m.isin([9,10,11])], ["winter","spring","summer","fall"], default="unk")
          df["_season"]=season

          # Rule: per GEOID×day×month×season if count >=2 → 1 else 0
          grp=df.groupby([gcol,"_day","_month","_season"]).size().rename("cnt").reset_index()
          grp["Y_label"]=(grp["cnt"]>=2).astype(int)
          df=df.merge(grp[[gcol,"_day","_month","_season","Y_label"]], on=[gcol,"_day","_month","_season"], how="left")
          out01=os.path.join(d,"sf_crime_01.csv")
          df.to_csv(out01, index=False)

          # keep latest _y.csv (upsert logic simplified here as overwrite with merged)
          df.drop(columns=["_day","_month","_season","cnt"], errors="ignore").to_csv(y_csv, index=False)

          # also parquet mirrors for artifact reuse
          pq.write_table(pa.Table.from_pandas(pd.read_csv(y_csv, low_memory=False)), os.path.join(d,"sf_crime_y.parquet"), compression=os.environ.get("Y_PARQUET_COMPRESSION","zstd"))
          pq.write_table(pa.Table.from_pandas(pd.read_csv(out01, low_memory=False)), os.path.join(d,"sf_crime_01.parquet"), compression=os.environ.get("Y_PARQUET_COMPRESSION","zstd"))
          PY

      # 2) 911 → sf_911_last_5_year_y.csv → join(sf_crime_01) → sf_crime_02.csv
      - name: 02) 911 integration → sf_crime_02.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          # normalize 911 source to *_y.csv
          for p in "${CRIME_DATA_DIR}/sf_911_last_5_year_y.csv" "${CRIME_DATA_DIR}/sf_911_last_5_year.csv" "sf_911_last_5_year_y.csv" "sf_911_last_5_year.csv"; do
            if [ -f "$p" ]; then cp -f "$p" "${CRIME_DATA_DIR}/sf_911_last_5_year_y.csv"; break; fi
          done
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq
          d=os.environ["CRIME_DATA_DIR"]
          a=pd.read_csv(os.path.join(d,"sf_crime_01.csv"), low_memory=False)
          b=pd.read_csv(os.path.join(d,"sf_911_last_5_year_y.csv"), low_memory=False)
          # naive ID-based left join by best-available keys (customize inside scripts)
          key=None
          for k in ["incident_id","case_number","id","service_request_id"]:
              if k in a.columns and k in b.columns: key=k; break
          if key is None:
              # fallback: join on GEOID + hour timestamp if exists
              for tc in ["incident_datetime","datetime","occurred_at","timestamp","date"]:
                  if tc in a.columns and tc in b.columns: tcol=tc; break
              else: tcol=None
              if tcol and "GEOID" in a.columns and "GEOID" in b.columns:
                  a["_ts"]=pd.to_datetime(a[tcol], errors="coerce")
                  b["_ts"]=pd.to_datetime(b[tcol], errors="coerce")
                  a["_ts_hour"]=a["_ts"].dt.floor("H"); b["_ts_hour"]=b["_ts"].dt.floor("H")
                  key=["GEOID","_ts_hour"]
              else:
                  key=None
          out=a.merge(b, how="left", on=key) if key is not None else a
          out.to_csv(os.path.join(d,"sf_crime_02.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_02.parquet"), compression="zstd")
          PY

      # 3) 311 → sf_311_last_5_years_y.csv → join(sf_crime_02) → sf_crime_03.csv
      - name: 03) 311 integration → sf_crime_03.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env: { CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} }
        shell: bash
        run: |
          set -euo pipefail
          for p in "${CRIME_DATA_DIR}/sf_311_last_5_years_y.csv" "${CRIME_DATA_DIR}/sf_311_last_5_years.csv" "sf_311_last_5_years_y.csv" "sf_311_last_5_years.csv"; do
            if [ -f "$p" ]; then cp -f "$p" "${CRIME_DATA_DIR}/sf_311_last_5_years_y.csv"; break; fi
          done
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq
          d=os.environ["CRIME_DATA_DIR"]
          a=pd.read_csv(os.path.join(d,"sf_crime_02.csv"), low_memory=False)
          b=pd.read_csv(os.path.join(d,"sf_311_last_5_years_y.csv"), low_memory=False)
          key=None
          for k in ["service_request_id","case_number","id"]:
              if k in a.columns and k in b.columns: key=k; break
          out=a.merge(b, how="left", on=key) if key else a
          out.to_csv(os.path.join(d,"sf_crime_03.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_03.parquet"), compression="zstd")
          PY

      # 4) Population (sf_population_y.csv) → join(sf_crime_03) → sf_crime_04.csv
      - name: 04) Population merge → sf_crime_04.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env: { CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} }
        shell: bash
        run: |
          set -euo pipefail
          for p in "${CRIME_DATA_DIR}/sf_population_y.csv" "${CRIME_DATA_DIR}/sf_population.csv" "sf_population_y.csv" "sf_population.csv"; do
            if [ -f "$p" ]; then cp -f "$p" "${CRIME_DATA_DIR}/sf_population_y.csv"; break; fi
          done
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq
          d=os.environ["CRIME_DATA_DIR"]
          pop=pd.read_csv(os.path.join(d,"sf_population_y.csv"), dtype=str, low_memory=False)
          a=pd.read_csv(os.path.join(d,"sf_crime_03.csv"), low_memory=False)
          # normalize GEOIDs
          geocol=None
          for c in ["GEOID","geoid","GEOID10","geography_id"]:
              if c in pop.columns: geocol=c; break
          if geocol is None: raise SystemExit("population GEOID missing")
          pop["GEOID10"]=pop[geocol].astype(str).str.strip().str.zfill(12).str[-10:]
          if "population" not in pop.columns:
              for c in ["total_population","b01003_001e","estimate","total","value"]:
                  if c in pop.columns: pop.rename(columns={c:"population"}, inplace=True); break
          pop["population"]=pd.to_numeric(pop["population"], errors="coerce").fillna(0)
          if "GEOID" not in a.columns: raise SystemExit("sf_crime_03.csv lacks GEOID")
          a["GEOID10"]=a["GEOID"].astype(str).str.zfill(10).str[-10:]
          out=a.drop(columns=["population"], errors="ignore").merge(pop[["GEOID10","population"]], on="GEOID10", how="left").drop(columns=["GEOID10"])
          out.to_csv(os.path.join(d,"sf_crime_04.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_04.parquet"), compression="zstd")
          PY

      # 5) Bus → sf_bus_stops_with_geoid_y.csv → join(sf_crime_04) → sf_crime_05.csv
      - name: 05) Bus features → sf_crime_05.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env: { CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} }
        shell: bash
        run: |
          set -euo pipefail
          for p in "${CRIME_DATA_DIR}/sf_bus_stops_with_geoid_y.csv" "${CRIME_DATA_DIR}/sf_bus_stops_with_geoid.csv" "sf_bus_stops_with_geoid_y.csv" "sf_bus_stops_with_geoid.csv"; do
            if [ -f "$p" ]; then cp -f "$p" "${CRIME_DATA_DIR}/sf_bus_stops_with_geoid_y.csv"; break; fi
          done
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq
          d=os.environ["CRIME_DATA_DIR"]
          a=pd.read_csv(os.path.join(d,"sf_crime_04.csv"), low_memory=False)
          bus=pd.read_csv(os.path.join(d,"sf_bus_stops_with_geoid_y.csv"), low_memory=False)
          # example aggregate per GEOID
          agg=bus.groupby("GEOID").size().rename("bus_stop_count").reset_index()
          out=a.merge(agg, how="left", on="GEOID")
          out.to_csv(os.path.join(d,"sf_crime_05.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_05.parquet"), compression="zstd")
          PY

      # 6) Train → sf_train_stops_with_geoid_y.csv → join(sf_crime_05) → sf_crime_06.csv
      - name: 06) Train features → sf_crime_06.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env: { CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} }
        shell: bash
        run: |
          set -euo pipefail
          for p in "${CRIME_DATA_DIR}/sf_train_stops_with_geoid_y.csv" "${CRIME_DATA_DIR}/sf_train_stops_with_geoid.csv" "sf_train_stops_with_geoid_y.csv" "sf_train_stops_with_geoid.csv"; do
            if [ -f "$p" ]; then cp -f "$p" "${CRIME_DATA_DIR}/sf_train_stops_with_geoid_y.csv"; break; fi
          done
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq
          d=os.environ["CRIME_DATA_DIR"]
          a=pd.read_csv(os.path.join(d,"sf_crime_05.csv"), low_memory=False)
          tr=pd.read_csv(os.path.join(d,"sf_train_stops_with_geoid_y.csv"), low_memory=False)
          agg=tr.groupby("GEOID").size().rename("train_stop_count").reset_index()
          out=a.merge(agg, how="left", on="GEOID")
          out.to_csv(os.path.join(d,"sf_crime_06.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_06.parquet"), compression="zstd")
          PY

      # 7) POI → sf_pois_cleaned_with_geoid_y.csv → join(sf_crime_06) → sf_crime_07.csv
      - name: 07) POI features → sf_crime_07.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env: { CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} }
        shell: bash
        run: |
          set -euo pipefail
          for p in "${CRIME_DATA_DIR}/sf_pois_cleaned_with_geoid_y.csv" "${CRIME_DATA_DIR}/sf_pois_cleaned_with_geoid.csv" "sf_pois_cleaned_with_geoid_y.csv" "sf_pois_cleaned_with_geoid.csv"; do
            if [ -f "$p" ]; then cp -f "$p" "${CRIME_DATA_DIR}/sf_pois_cleaned_with_geoid_y.csv"; break; fi
          done
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq
          d=os.environ["CRIME_DATA_DIR"]
          a=pd.read_csv(os.path.join(d,"sf_crime_06.csv"), low_memory=False)
          poi=pd.read_csv(os.path.join(d,"sf_pois_cleaned_with_geoid_y.csv"), low_memory=False)
          agg=poi.groupby(["GEOID","poi_category"]).size().unstack(fill_value=0)
          agg.columns=[f"poi_{c}_cnt" for c in agg.columns]
          agg=agg.reset_index()
          out=a.merge(agg, how="left", on="GEOID")
          out.to_csv(os.path.join(d,"sf_crime_07.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_07.parquet"), compression="zstd")
          PY

      # 8) Police & Gov → sf_police_stations_y.csv → join(sf_crime_07) → sf_crime_08.csv
      - name: 08) Police & Gov features → sf_crime_08.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env: { CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} }
        shell: bash
        run: |
          set -euo pipefail
          for p in "${CRIME_DATA_DIR}/sf_police_stations_y.csv" "${CRIME_DATA_DIR}/sf_police_stations.csv" "sf_police_stations_y.csv" "sf_police_stations.csv"; do
            if [ -f "$p" ]; then cp -f "$p" "${CRIME_DATA_DIR}/sf_police_stations_y.csv"; break; fi
          done
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq
          d=os.environ["CRIME_DATA_DIR"]
          a=pd.read_csv(os.path.join(d,"sf_crime_07.csv"), low_memory=False)
          pol=pd.read_csv(os.path.join(d,"sf_police_stations_y.csv"), low_memory=False)
          agg=pol.groupby("GEOID").size().rename("police_site_count").reset_index()
          out=a.merge(agg, how="left", on="GEOID")
          out.to_csv(os.path.join(d,"sf_crime_08.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_08.parquet"), compression="zstd")
          PY

      # 9) Weather → sf_weather_5years_y.csv → join(sf_crime_08) → sf_crime_09.csv
      - name: 09) Weather features → sf_crime_09.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env: { CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} }
        shell: bash
        run: |
          set -euo pipefail
          for p in "${CRIME_DATA_DIR}/sf_weather_5years_y.csv" "${CRIME_DATA_DIR}/sf_weather_5years.csv" "sf_weather_5years_y.csv" "sf_weather_5years.csv"; do
            if [ -f "$p" ]; then cp -f "$p" "${CRIME_DATA_DIR}/sf_weather_5years_y.csv"; break; fi
          done
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq
          d=os.environ["CRIME_DATA_DIR"]
          a=pd.read_csv(os.path.join(d,"sf_crime_08.csv"), low_memory=False)
          wx=pd.read_csv(os.path.join(d,"sf_weather_5years_y.csv"), low_memory=False)
          # simple hour-level merge if possible
          tcol=None
          for c in ["incident_datetime","datetime","occurred_at","timestamp","date"]:
              if c in a.columns and c in wx.columns: tcol=c; break
          if tcol:
              a["_ts"]=pd.to_datetime(a[tcol], errors="coerce").dt.floor("H")
              wx["_ts"]=pd.to_datetime(wx[tcol], errors="coerce").dt.floor("H")
              key=["GEOID","_ts"] if "GEOID" in wx.columns else ["_ts"]
              out=a.merge(wx, how="left", on=key).drop(columns=["_ts"])
          else:
              out=a
          out.to_csv(os.path.join(d,"sf_crime_09.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_09.parquet"), compression="zstd")
          PY

      # 10) Neighbor windows (24h/3d/7d/30d/365d) → sf_crime_10.csv
      - name: 10) Neighbor effects → sf_crime_10.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env: { CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} }
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os,pandas as pd,pyarrow as pa,pyarrow.parquet as pq,numpy as np
          d=os.environ["CRIME_DATA_DIR"]
          df=pd.read_csv(os.path.join(d,"sf_crime_09.csv"), low_memory=False)
          # required: neighbors.csv with columns (GEOID, NEIGHBOR_GEOID)
          neigh_path=os.path.join(d,"neighbors.csv")
          if os.path.exists(neigh_path):
              nb=pd.read_csv(neigh_path, dtype=str)
          else:
              nb=pd.DataFrame(columns=["GEOID","NEIGHBOR_GEOID"])
          # time column
          tcol=None
          for c in ["incident_datetime","datetime","occurred_at","timestamp","date"]:
              if c in df.columns: tcol=c; break
          ts=pd.to_datetime(df[tcol], errors="coerce").dt.tz_localize("UTC", nonexistent="shift_forward", ambiguous="NaT").dt.tz_convert("America/Los_Angeles") if df[tcol].dtype==object else pd.to_datetime(df[tcol], errors="coerce")
          df["_ts"]=ts
          # base counts per GEOID×time
          base=df[["GEOID","_ts"]].copy()
          base["_ts_hour"]=base["_ts"].dt.floor("H")
          counts=base.groupby(["GEOID","_ts_hour"]).size().rename("cnt").reset_index()

          # neighbor expand
          if len(nb):
              # build neighbor map
              m=nb.groupby("GEOID")["NEIGHBOR_GEOID"].apply(list)
          else:
              m=pd.Series(dtype=object)

          def add_window(hours,name):
              # self counts
              c=counts.copy()
              # rolling sum over past window (exclude current hour)
              c=c.sort_values(["GEOID","_ts_hour"])
              # expand to all hours present
              idx=pd.MultiIndex.from_product([c["GEOID"].unique(), c["_ts_hour"].unique()], names=["GEOID","_ts_hour"])
              c=c.set_index(["GEOID","_ts_hour"]).reindex(idx, fill_value=0).sort_index()
              r=c.groupby(level=0)["cnt"].rolling(f"{hours}H", closed="left").sum().reset_index().rename(columns={"cnt":f"self_{name}"})
              return r

          windows=[(24,"24h"),(72,"3d"),(168,"7d"),(720,"30d"),(8760,"365d")]
          out=df.copy()
          out["_ts_hour"]=out["_ts"].dt.floor("H")
          out=out.drop(columns=[c for c in ["neighbor_24h","neighbor_3d","neighbor_7d","neighbor_30d","neighbor_365d"] if c in out.columns], errors="ignore")

          # self windows (optional; keeping only neighbor_* as requested)
          # neighbor windows
          for H,name in windows:
              col=f"neighbor_{name}"
              # approximate via joining neighbor lists
              if len(m):
                  nb_rows=[]
                  for g, sub in counts.groupby("GEOID"):
                      hrs=sub["_ts_hour"].unique()
                      nbs=m.get(g, [])
                      if not len(nbs): continue
                      ref=counts[counts["GEOID"].isin(nbs)]
                      tmp=ref[["_ts_hour","cnt"]].groupby("_ts_hour").sum().reindex(hrs, fill_value=0).sort_index()
                      # rolling
                      s=tmp["cnt"].rolling(f"{H}H", closed="left").sum()
                      nb_rows.append(pd.DataFrame({"GEOID":g,"_ts_hour":s.index, col:s.values}))
                  if nb_rows:
                      nb_df=pd.concat(nb_rows, ignore_index=True)
                      out=out.merge(nb_df, how="left", on=["GEOID","_ts_hour"])
                  else:
                      out[col]=np.nan
              else:
                  out[col]=np.nan

          out.to_csv(os.path.join(d,"sf_crime_10.csv"), index=False)
          pq.write_table(pa.Table.from_pandas(out), os.path.join(d,"sf_crime_10.parquet"), compression="zstd")
          PY

      # 11) Stacking on sf_crime_10.csv → outputs
      - name: 11) Stacking
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          PATROL_TOP_K:   ${{ env.PATROL_TOP_K }}
        shell: bash
        run: |
          set -euo pipefail
          test -f "${CRIME_DATA_DIR}/sf_crime_10.csv" || { echo "❌ sf_crime_10.csv yok"; exit 2; }
          python - <<'PY'
          import os,pandas as pd,pyarrow.parquet as pq,pyarrow as pa
          d=os.environ["CRIME_DATA_DIR"]
          df=pd.read_csv(os.path.join(d,"sf_crime_10.csv"), low_memory=False)
          # placeholder: risk score = normalized recent neighbor_24h
          import numpy as np
          s=df.get("neighbor_24h", pd.Series(np.zeros(len(df))))
          rs=(s - s.min())/(s.max()-s.min()+1e-9)
          df["risk_score"]=rs
          df.to_parquet(os.path.join(d,"risk_hourly.parquet"), index=False)
          df.head(0).to_parquet(os.path.join(d,"patrol_recs_multi.parquet"), index=False)  # placeholder
          PY

      # ===== Previews =====
      - name: Quick preview (parquet mirrors)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        shell: bash
        run: |
          python - <<'PY'
          import os,polars as pl
          d=os.environ["CRIME_DATA_DIR"]
          for p in ["sf_crime_y.parquet","sf_crime_01.parquet","sf_crime_02.parquet","sf_crime_03.parquet","sf_crime_04.parquet","sf_crime_05.parquet","sf_crime_06.parquet","sf_crime_07.parquet","sf_crime_08.parquet","sf_crime_09.parquet","sf_crime_10.parquet","risk_hourly.parquet"]:
              f=os.path.join(d,p)
              if os.path.exists(f):
                  print("=== ",p," ===")
                  try: print(pl.scan_parquet(f).head(5).collect())
                  except Exception as e: print("preview error:", e)
          PY

      # ---- Outputs (zip parquet for artifact reuse) ----
      - name: Zip parquet outputs into single artifact
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          ts=$(date +%Y%m%d)
          mkdir -p upload
          zip -r upload/sf-crime-parquet-${ts}.zip "${CRIME_DATA_DIR}"/*.parquet || true
          ls -lah upload || true

      - name: Upload artifact (zip + loose parquet)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: sf-crime-pipeline-output
          path: |
            upload/*.zip
            ${{ env.CRIME_DATA_DIR }}/*.parquet
          retention-days: 14

      - name: Commit & push (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' && github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'commit' }}
        run: |
          set -e
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "🔄 Full pipeline (CSV → _y.csv priority + Parquet artifact) [skip ci]" || echo "No changes"
          git push || echo "Nothing to push"
