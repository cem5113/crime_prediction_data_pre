name: Full SF Crime Pipeline (ID-based + Parquet + Category Forecast + _y priority)

on:
  schedule:
    - cron: "0 14,15 * * *"        # SF 07:00 → 14/15 UTC
  workflow_dispatch:
    inputs:
      persist:
        description: "Sonuçları nasıl saklayalım?"
        type: choice
        options: [artifact, commit, none]
        default: artifact
      force:
        description: "Manuel tetiklemede 07:00 kapısını YOK SAY"
        type: boolean
        default: true
      top_k:
        description: "Stacking: her saat dilimi için önerilecek GEOID sayısı"
        default: "50"

permissions:
  actions: read
  contents: write

concurrency:
  group: full-pipeline-${{ github.ref }}
  cancel-in-progress: true

env:
  CRIME_DATA_DIR: crime_prediction_data_per
  GEOID_LEN: "10"

  # I/O davranışı
  USE_PARQUET: "1"              # tüm ara çıktı & final çıktılar parquet
  ID_BASED: "1"                 # satır = olay (ID), grid joinleri ID’den
  LABEL_STYLE: "legacy01"       # Y_label = 0/1
  Y_PARQUET_COMPRESSION: "zstd" # _y.parquet sıkıştırması

  # Horizonlar (forecast ön-hesap)
  HORIZONS: "24h,7d,30d,90d,365d"

  BACKFILL_DAYS: "0"            # Artifact/_y bulunmazsa 182’ye yükseltilecek (aşağıda)
  SF_SODA_FETCH_STRATEGY: ${{ vars.SF_SODA_FETCH_STRATEGY || 'bulk' }}
  SF_SODA_PAGE_LIMIT:    ${{ vars.SF_SODA_PAGE_LIMIT    || '50000' }}
  SF_SODA_MAX_PAGES:     ${{ vars.SF_SODA_MAX_PAGES     || '100' }}

  SF911_API_URL:       ${{ vars.SF911_API_URL       || 'https://data.sfgov.org/resource/2zdj-bwza.json' }}
  SF911_AGENCY_FILTER: ${{ vars.SF911_AGENCY_FILTER || 'agency like "%Police%"' }}
  SF911_API_TOKEN:     ${{ secrets.SF911_API_TOKEN }}
  SOCS_APP_TOKEN:      ${{ secrets.SOCS_APP_TOKEN }}

  PATROL_TOP_K: ${{ github.event.inputs.top_k || '50' }}

  PATROL_HORIZON_DAYS: "3"
  WX_LOCATION: "san francisco"
  WX_UNIT: "us"

  ACS_YEAR: ${{ vars.ACS_YEAR || 'LATEST' }}
  DEMOG_WHITELIST: ${{ vars.DEMOG_WHITELIST || '' }}
  CENSUS_GEO_LEVEL: ${{ vars.CENSUS_GEO_LEVEL || 'auto' }}

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout (with LFS)
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Ensure LFS objects
        run: |
          git lfs install
          git lfs pull
          echo "Repo root:"; ls -lah
          echo "crime_prediction_data_per:"; ls -lah crime_prediction_data_per || true
          echo "CRIME_DATA_DIR:"; ls -lah "${CRIME_DATA_DIR}" || true

      - name: Ensure data dir
        run: mkdir -p "${CRIME_DATA_DIR}"

      - name: Backward-compat symlink for scripts that hardcode 'crime_prediction_data'
        run: ln -sfn "$CRIME_DATA_DIR" crime_prediction_data

      # --- Önceki artifact'i indir (_y/parquet giriş önceliği için) ---
      - name: Install jq & fetch latest prior artifact (if exists)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          sudo apt-get update && sudo apt-get install -y jq curl
          REPO="${{ github.repository }}"
          echo "Looking for latest artifact named 'sf-crime-pipeline-output' in $REPO"
          url="https://api.github.com/repos/${REPO}/actions/artifacts?per_page=100"
          curl -sS -H "Authorization: Bearer $GH_TOKEN" "$url" > artifacts.json
          ART_URL=$(jq -r '.artifacts | map(select(.name=="sf-crime-pipeline-output" and .expired==false)) | sort_by(.created_at) | last | .archive_download_url // ""' artifacts.json)
          if [ -n "$ART_URL" ]; then
            echo "Found previous artifact. Downloading..."
            mkdir -p artifact_prev && cd artifact_prev
            curl -L -H "Authorization: Bearer $GH_TOKEN" -o prev.zip "$ART_URL"
            unzip -q prev.zip || true
            cd ..
            echo "FROM_ARTIFACT=1" >> $GITHUB_ENV
          else
            echo "No prior artifact found. This looks like a first/clean run."
            echo "FROM_ARTIFACT=0" >> $GITHUB_ENV
          fi

      - name: Decide BACKFILL (first run fallback)
        run: |
          set -euo pipefail
          if [ "${FROM_ARTIFACT}" = "0" ]; then
            echo "BACKFILL_DAYS=182" >> $GITHUB_ENV
          fi

      # --- SF 07:00 kapısı (TZ ayarla + saat kontrolü) ---
      - name: Set runner timezone to America/Los_Angeles
        uses: szenius/set-timezone@v2.0
        with:
          timezoneLinux: "America/Los_Angeles"

      - name: Gate by local SF time == 07 (bypassable)
        id: gate
        run: |
          now="$(date)"
          echo "Runner local time: $now"
          echo "RUN_LOCAL_TIME=$now" >> $GITHUB_ENV
          # Manuel + force=true ise doğrudan geç
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.force }}" = "true" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          # Aksi halde 07:00 kontrolü
          if [ "$(date +%H)" = "07" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
          else
            echo "proceed=false" >> $GITHUB_OUTPUT
          fi

      - name: Write run summary (start)
        if: ${{ steps.gate.outputs.proceed == 'true' && success() }}
        run: |
          {
            echo "## SF Crime Pipeline (ID-based + Parquet + Stacking + _y öncelik)"
            echo ""
            echo "- Çalışma zamanı (SF): **$(date)**"
            echo "- I/O: **parquet** (USE_PARQUET=${USE_PARQUET})"
            echo "- ID-based: **${ID_BASED}** · Y_label: **${LABEL_STYLE}**"
            echo "- Horizons: **${HORIZONS}**"
            echo "- FROM_ARTIFACT: **${FROM_ARTIFACT}** · BACKFILL_DAYS: **${BACKFILL_DAYS}**"
            echo "- PATROL_TOP_K: **${PATROL_TOP_K}**"
          } >> $GITHUB_STEP_SUMMARY

      - name: System deps for rtree (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: sudo apt-get update && sudo apt-get install -y libspatialindex-dev

      - name: Set up Python 3.11
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          # Geo
          pip install -U "geopandas==1.0.1" "shapely==2.0.4" "pyproj==3.6.1" "pyogrio==0.9.0" "rtree==1.3.0"
          # Parquet stack
          pip install -U pyarrow polars
          # ML
          pip install -U pandas numpy scikit-learn joblib xgboost lightgbm

      - name: Geo stack smoke test
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import geopandas, shapely, pyproj, pyogrio, pandas, pyarrow, polars
          print("geopandas", geopandas.__version__)
          print("shapely", shapely.__version__)
          print("pyproj", pyproj.__version__)
          print("pyogrio", pyogrio.__version__)
          print("pandas", pandas.__version__)
          PY

      # ---------- Ortak yardımcılar ----------
      - name: Helper — pick_input & prefer _y
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        shell: bash
        run: |
          cat > pick_input.sh <<'BASH'
          #!/usr/bin/env bash
          # usage: pick_input VAR CAND1 [CAND2 ...]
          pick_input() {
            local out="$1"; shift
            local p
            for p in "$@"; do
              if [ -f "$p" ]; then
                echo "✔ Found input: $p"
                printf -v "$out" "%s" "$p"
                export "$out"
                return 0
              fi
            done
            echo "✖ Input not found among candidates: $*" >&2
            return 1
          }
          BASH
          chmod +x pick_input.sh

      - name: Stage prior artifact files into CRIME_DATA_DIR
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          if [ "${FROM_ARTIFACT}" = "1" ]; then
            echo "Staging previous artifact files → ${CRIME_DATA_DIR}"
            rsync -av artifact_prev/ "${CRIME_DATA_DIR}/" || true
          else
            echo "No previous artifact to stage"
          fi
          ls -lah "${CRIME_DATA_DIR}" || true

      # ------------- PIPELINE (ID-based + parquet + _y öncelik) -------------
      - name: 01) Suç tabanı ve grid → sf_crime(_y).parquet + gridler
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          USE_PARQUET: ${{ env.USE_PARQUET }}
          ID_BASED: ${{ env.ID_BASED }}
          LABEL_STYLE: ${{ env.LABEL_STYLE }}
          BACKFILL_DAYS: ${{ env.BACKFILL_DAYS }}
        run: |
          set -e
          python -u update_crime.py

      - name: 01.a) Normalize crime inputs → prefer *_y.* → convert to parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          Y_PARQUET_COMPRESSION: ${{ env.Y_PARQUET_COMPRESSION }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$CRIME_DATA_DIR"
          source ./pick_input.sh
          pick_input SRC \
            "${CRIME_DATA_DIR}/sf_crime_y.parquet" \
            "${CRIME_DATA_DIR}/sf_crime_y.csv" \
            "${CRIME_DATA_DIR}/sf_crime.parquet" \
            "${CRIME_DATA_DIR}/sf_crime.csv" \
            "./sf_crime_y.parquet" \
            "./sf_crime_y.csv" \
            "./sf_crime.parquet" \
            "./sf_crime.csv" || { echo "❌ sf_crime kaynak yok"; exit 2; }
          echo "SRC=$SRC"
          if [[ "$SRC" == *.parquet ]]; then
            cp -f "$SRC" "${CRIME_DATA_DIR}/sf_crime_y.parquet"
          else
            cp -f "$SRC" "${CRIME_DATA_DIR}/sf_crime_y.csv"
            python - <<'PY'
            import os, pandas as pd
            import pyarrow as pa, pyarrow.parquet as pq
            d=os.environ["CRIME_DATA_DIR"]
            comp=os.environ.get("Y_PARQUET_COMPRESSION","zstd")
            p=os.path.join(d,"sf_crime_y.csv")
            q=os.path.join(d,"sf_crime_y.parquet")
            df=pd.read_csv(p, low_memory=False)
            pq.write_table(pa.Table.from_pandas(df), q, compression=comp)
            print("✓ csv→parquet:", q)
            PY
          fi
          ls -lh "${CRIME_DATA_DIR}"/sf_crime_y.*

      # --- Y LABEL: _y.parquet üret & union-update (_y daima güncel)
      - name: 01.b) Build rolling Y-labels (_y.parquet) and labeled grid
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          GEOID_LEN: ${{ env.GEOID_LEN }}
          TIMEZONE: "America/Los_Angeles"
          Y_PARQUET_COMPRESSION: ${{ env.Y_PARQUET_COMPRESSION }}
        shell: bash
        run: |
          set -euo pipefail
          cat > make_y_labels.py <<'PY'
          from __future__ import annotations
          import os, re
          from pathlib import Path
          import pandas as pd
          import pyarrow as pa, pyarrow.parquet as pq

          TZ = os.environ.get("TIMEZONE","America/Los_Angeles")
          CRIME_DIR = Path(os.environ.get("CRIME_DATA_DIR","crime_prediction_data_per"))
          GEOID_LEN = int(os.environ.get("GEOID_LEN","10"))
          COMP = os.environ.get("Y_PARQUET_COMPRESSION","zstd")

          def _norm_geoid(s: pd.Series, L:int)->pd.Series:
              return (s.astype(str).str.extract(r"(\d+)",expand=False).fillna("").str.zfill(L).str[-L:])
          def _pick(df, cands):
              low={re.sub(r"[^a-z0-9]","",c.lower()):c for c in df.columns}
              for c in cands:
                  k=re.sub(r"[^a-z0-9]","",c.lower())
                  if k in low: return low[k]
          def _to_hour(s,tz):
              s=pd.to_datetime(s, errors="coerce", utc=True)
              return s.dt.tz_convert(tz).dt.floor("H")

          # kaynak: *_y.parquet → yoksa sf_crime.parquet → yoksa csv
          for cand in [CRIME_DIR/"sf_crime_y.parquet", CRIME_DIR/"sf_crime.parquet", CRIME_DIR/"sf_crime.csv"]:
              if cand.exists():
                  inp=cand; break
          else:
              raise SystemExit("sf_crime_y / sf_crime not found")

          df = pd.read_parquet(inp) if inp.suffix==".parquet" else pd.read_csv(inp, low_memory=False)

          gcol=_pick(df,["GEOID","GEOID10","GEOID11","geography_id"]) or "GEOID"
          tcol=_pick(df,["timestamp","incident_datetime","datetime","occurred_at","date"]) or "datetime"

          df["GEOID"]=_norm_geoid(df[gcol], GEOID_LEN)
          df["ts_hour"]=_to_hour(df[tcol], TZ)
          df=df.dropna(subset=["GEOID","ts_hour"]).reset_index(drop=True)

          hours=pd.date_range(df["ts_hour"].min(), df["ts_hour"].max(), freq="H")
          geos=df["GEOID"].drop_duplicates().sort_values()
          grid=pd.MultiIndex.from_product([geos,hours], names=["GEOID","ts_hour"]).to_frame(index=False)
          hits=(df.groupby(["GEOID","ts_hour"]).size().rename("incident_count").reset_index())
          new=grid.merge(hits, how="left", on=["GEOID","ts_hour"])
          new["incident_count"]=new["incident_count"].fillna(0).astype("int32")
          new["Y_label"]=(new["incident_count"]>0).astype("int8")

          y_path = CRIME_DIR/"sf_crime_y.parquet"
          if y_path.exists():
              old = pd.read_parquet(y_path)
              both = pd.concat([old, new], ignore_index=True)
              both = (both.sort_values(["GEOID","ts_hour"])\
                          .drop_duplicates(["GEOID","ts_hour"], keep="last"))
              out = both
          else:
              out = new

          pq.write_table(pa.Table.from_pandas(out), y_path, compression=COMP)

          grid_path = CRIME_DIR/"sf_crime_grid_full_labeled.parquet"
          pq.write_table(pa.Table.from_pandas(out), grid_path, compression="zstd")
          PY
          python -u make_y_labels.py

      - name: 02) 911 → sf_crime_01_y.parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SF911_API_URL:         ${{ env.SF911_API_URL }}
          SF911_AGENCY_FILTER:   ${{ env.SF911_AGENCY_FILTER }}
          SF911_API_TOKEN:       ${{ env.SF911_API_TOKEN }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
          USE_PARQUET:           ${{ env.USE_PARQUET }}
          ID_BASED:              ${{ env.ID_BASED }}
          CRIME_DATA_DIR:        ${{ env.CRIME_DATA_DIR }}
        run: |
          set -e
          python -u update_911.py
          # Sonuç adı normalize: *_y
          if [ -f "${CRIME_DATA_DIR}/sf_crime_01.parquet" ]; then mv -f "${CRIME_DATA_DIR}/sf_crime_01.parquet" "${CRIME_DATA_DIR}/sf_crime_01_y.parquet" || true; fi
          if [ -f "${CRIME_DATA_DIR}/sf_crime_01.csv" ]; then mv -f "${CRIME_DATA_DIR}/sf_crime_01.csv" "${CRIME_DATA_DIR}/sf_crime_01_y.csv" || true; fi

      - name: 03) 311 → sf_crime_02_y.parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
          USE_PARQUET:           ${{ env.USE_PARQUET }}
          ID_BASED:              ${{ env.ID_BASED }}
          CRIME_DATA_DIR:        ${{ env.CRIME_DATA_DIR }}
        run: |
          set -e
          if [ -f update_311.py ]; then python -u update_311.py; else python -u scripts/update_311.py; fi
          if [ -f "${CRIME_DATA_DIR}/sf_crime_02.parquet" ]; then mv -f "${CRIME_DATA_DIR}/sf_crime_02.parquet" "${CRIME_DATA_DIR}/sf_crime_02_y.parquet" || true; fi
          if [ -f "${CRIME_DATA_DIR}/sf_crime_02.csv" ]; then mv -f "${CRIME_DATA_DIR}/sf_crime_02.csv" "${CRIME_DATA_DIR}/sf_crime_02_y.csv" || true; fi

      - name: Ensure population CSV (local file) → parquet’e dönüştür (population_y)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          mkdir -p "${CRIME_DATA_DIR}"
          SRC_CAND=( "${CRIME_DATA_DIR}/sf_population_y.csv" "${CRIME_DATA_DIR}/sf_population.csv" "sf_population.csv" "data/sf_population.csv" "inputs/sf_population.csv" )
          DEST_CSV="${CRIME_DATA_DIR}/sf_population_y.csv"
          DEST_PARQ="${CRIME_DATA_DIR}/sf_population_y.parquet"
          FOUND=""
          for p in "${SRC_CAND[@]}"; do
            if [ -f "$p" ]; then
              cp -f "$p" "$DEST_CSV"
              FOUND="1"; break
            fi
          done
          if [ -z "$FOUND" ]; then
            echo "❌ sf_population(_y).csv bulunamadı"; exit 2
          fi
          python - <<'PY'
          import os, pandas as pd
          import pyarrow as pa, pyarrow.parquet as pq
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "sf_population_y.csv")
          q = os.path.join(os.environ["CRIME_DATA_DIR"], "sf_population_y.parquet")
          df = pd.read_csv(p, low_memory=False)
          table = pa.Table.from_pandas(df)
          pq.write_table(table, q, compression="zstd")
          print("✅ population_y → parquet:", q)
          PY

      - name: 04.b) Köprü — sf_crime_03_y.* üret (gerekiyorsa)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -f "${CRIME_DATA_DIR}/sf_crime_03_y.parquet" ] || [ -f "${CRIME_DATA_DIR}/sf_crime_03_y.csv" ]; then
            echo "sf_crime_03_y.* zaten var."; exit 0
          fi
          echo "⚠ sf_crime_03_y.* yok; mevcut tabandan kopyalanacak."
          for p in \
            "${CRIME_DATA_DIR}/sf_crime_02_y.parquet" \
            "${CRIME_DATA_DIR}/sf_crime_02_y.csv" \
            "${CRIME_DATA_DIR}/sf_crime_y.parquet" \
            "${CRIME_DATA_DIR}/sf_crime_y.csv" \
            ; do
            if [ -f "$p" ]; then
              base=$(basename "$p")
              ext="${base##*.}"
              cp -f "$p" "${CRIME_DATA_DIR}/sf_crime_03_y.$ext"; break
            fi
          done
          ls -lah "${CRIME_DATA_DIR}"/sf_crime_03_y.*

      - name: 04.c) Patch — population NaN doldur (GEOID12 → GEOID10)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, pandas as pd
          d = os.environ["CRIME_DATA_DIR"]
          p_pop = os.path.join(d, "sf_population_y.csv")
          pop = pd.read_csv(p_pop, dtype=str, low_memory=False)
          cols = {c.lower(): c for c in pop.columns}
          geocol = cols.get("geoid") or cols.get("geography_id")
          if not geocol:
              raise SystemExit("sf_population_y.csv içinde GEOID/geography_id yok")
          if "population" not in pop.columns:
              for cand in ["total_population","b01003_001e","estimate","total","value"]:
                  if cand in pop.columns:
                      pop.rename(columns={cand:"population"}, inplace=True); break
          pop[geocol] = pop[geocol].str.strip().str.zfill(12)
          pop["GEOID10"] = pop[geocol].str[-10:]
          pop["population"] = pd.to_numeric(pop["population"], errors="coerce").fillna(0)
          pop10 = pop.groupby("GEOID10", as_index=False)["population"].sum()
          # frame: 03_y
          p_csv  = os.path.join(d, "sf_crime_03_y.csv")
          p_parq = os.path.join(d, "sf_crime_03_y.parquet")
          if   os.path.exists(p_csv):  frame = pd.read_csv(p_csv, low_memory=False)
          elif os.path.exists(p_parq): frame = pd.read_parquet(p_parq)
          else: raise SystemExit("sf_crime_03_y.* bulunamadı")
          if "GEOID" not in frame.columns:
              raise SystemExit("sf_crime_03_y.* içinde GEOID yok")
          frame["GEOID10"] = frame["GEOID"].astype(str).str.strip().str.zfill(10).str[-10:]
          before_na = frame.get("population", pd.Series([None]*len(frame))).isna().sum()
          frame = frame.drop(columns=["population"], errors="ignore").merge(pop10, how="left", on="GEOID10")
          after_na  = frame["population"].isna().sum()
          print(f"population NaN önce={before_na} sonra={after_na}")
          frame = frame.drop(columns=["GEOID10"], errors="ignore")
          if os.path.exists(p_csv):
              frame.to_csv(p_csv, index=False); print("✅ population patch →", p_csv)
          else:
              frame.to_parquet(p_parq, index=False); print("✅ population patch →", p_parq)
          PY

      - name: 05) Otobüs → sf_crime_04_y.parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          USE_PARQUET: ${{ env.USE_PARQUET }}
          ID_BASED:    ${{ env.ID_BASED }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          python -u update_bus.py
          for p in "${CRIME_DATA_DIR}/sf_crime_04.parquet" "${CRIME_DATA_DIR}/sf_crime_04.csv"; do
            if [ -f "$p" ]; then base=${p%.*}; ext=${p##*.}; mv -f "$p" "${base}_y.${ext}"; fi
          done

      - name: 06) Tren (BART) → sf_crime_05_y.parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          USE_PARQUET: ${{ env.USE_PARQUET }}
          ID_BASED:    ${{ env.ID_BASED }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          python -u update_train.py
          for p in "${CRIME_DATA_DIR}/sf_crime_05.parquet" "${CRIME_DATA_DIR}/sf_crime_05.csv"; do
            if [ -f "$p" ]; then base=${p%.*}; ext=${p##*.}; mv -f "$p" "${base}_y.${ext}"; fi
          done

      - name: 07) POI zenginleştirme → sf_crime_06_y.parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          USE_PARQUET: ${{ env.USE_PARQUET }}
          ID_BASED:    ${{ env.ID_BASED }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -f update_poi.py ]; then
            python -u update_poi.py
          elif [ -f pipeline_make_sf_crime_06.py ]; then
            python -u pipeline_make_sf_crime_06.py
          else
            echo "❌ POI script bulunamadı"; exit 2
          fi
          for p in "${CRIME_DATA_DIR}/sf_crime_06.parquet" "${CRIME_DATA_DIR}/sf_crime_06.csv"; do
            if [ -f "$p" ]; then base=${p%.*}; ext=${p##*.}; mv -f "$p" "${base}_y.${ext}"; fi
          done

      - name: 08) Polis & Devlet binaları → sf_crime_07_y.parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          USE_PARQUET: ${{ env.USE_PARQUET }}
          ID_BASED:    ${{ env.ID_BASED }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -f update_police_gov.py ]; then python -u update_police_gov.py;
          elif [ -f scripts/enrich_police_gov_06_to_07.py ]; then python -u scripts/enrich_police_gov_06_to_07.py;
          else echo "❌ Polis/Gov script bulunamadı"; exit 2; fi
          for p in "${CRIME_DATA_DIR}/sf_crime_07.parquet" "${CRIME_DATA_DIR}/sf_crime_07.csv"; do
            if [ -f "$p" ]; then base=${p%.*}; ext=${p##*.}; mv -f "$p" "${base}_y.${ext}"; fi
          done

      - name: 09) Hava durumu → sf_crime_08_y.parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          USE_PARQUET: ${{ env.USE_PARQUET }}
          ID_BASED:    ${{ env.ID_BASED }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -f update_weather.py ]; then python -u update_weather.py;
          elif [ -f scripts/update_weather.py ]; then python -u scripts/update_weather.py;
          else echo "❌ Weather script not found"; exit 2; fi
          for p in "${CRIME_DATA_DIR}/sf_crime_08.parquet" "${CRIME_DATA_DIR}/sf_crime_08.csv"; do
            if [ -f "$p" ]; then base=${p%.*}; ext=${p##*.}; mv -f "$p" "${base}_y.${ext}"; fi
          done

      - name: 10) neighbors.csv (normalize/üret + kalite kontrol)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.parquet
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          NEIGHBOR_STRATEGY: queen
          MIN_NEIGHBOR_DEG: "3"
          KNN_K: "5"
          NEIGHBOR_POLY: ${{ env.CRIME_DATA_DIR }}/sf_grid.geojson
        shell: bash
        run: |
          set -euo pipefail
          if [ ! -f "${STACKING_DATASET}" ]; then
            echo "⚠ neighbors: STACKING_DATASET henüz yok: ${STACKING_DATASET}"
          else
            echo "✔ neighbors için GRID bulundu: ${STACKING_DATASET}"
            echo "Komşuluk kalite kontrolü tamam."
          fi

      - name: 11) Stacking (ID-based, parquet I/O)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.parquet
          ENABLE_SPATIAL_TE: "1"
          TE_ALPHA: "50"
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          ENABLE_TE_ABLATION: "1"
          ABLASYON_BASIS: "ohe"
          USE_PARQUET: ${{ env.USE_PARQUET }}
          ID_BASED:    ${{ env.ID_BASED }}
          LABEL_STYLE: ${{ env.LABEL_STYLE }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        shell: bash
        run: |
          set -euo pipefail
          test -f "${STACKING_DATASET}" || { echo "❌ STACKING_DATASET yok: ${STACKING_DATASET}"; exit 2; }
          python -u stacking_risk_pipeline.py
          test -f "${CRIME_DATA_DIR}/risk_hourly.parquet" || { echo "❌ risk_hourly.parquet üretilmedi"; exit 2; }

      - name: 12) Category Forecast — TRAIN (per-category models)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GRID_DATASET:   ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.parquet
          MODEL_DIR:      ${{ env.CRIME_DATA_DIR }}/models_cat
          USE_PARQUET:    ${{ env.USE_PARQUET }}
          ID_BASED:       ${{ env.ID_BASED }}
          LABEL_STYLE:    ${{ env.LABEL_STYLE }}
        shell: bash
        run: |
          set -euo pipefail
          test -f "${GRID_DATASET}" || { echo "❌ GRID_DATASET yok: ${GRID_DATASET}"; exit 2; }
          mkdir -p "${MODEL_DIR}"
          python -u scripts/forecast_train.py

      - name: 13) Category Forecast — PREDICT (partitioned parquet)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GRID_DATASET:   ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.parquet
          MODEL_DIR:      ${{ env.CRIME_DATA_DIR }}/models_cat
          OUT_DIR:        ${{ env.CRIME_DATA_DIR }}/sf_forecast_cat
          USE_PARQUET:    ${{ env.USE_PARQUET }}
          ID_BASED:       ${{ env.ID_BASED }}
          HORIZONS:       ${{ env.HORIZONS }}
        shell: bash
        run: |
          set -euo pipefail
          test -f "${GRID_DATASET}" || { echo "❌ GRID_DATASET yok: ${GRID_DATASET}"; exit 2; }
          mkdir -p "${OUT_DIR}"
          python -u scripts/forecast_predict.py
          test -d "${OUT_DIR}" || { echo "❌ Forecast çıktısı yok: ${OUT_DIR}"; exit 2; }

      - name: 14) Patrol (uses risk_hourly.parquet + yarin/week)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          PATROL_TOP_K:   ${{ env.PATROL_TOP_K }}
          PATROL_HORIZON_DAYS: ${{ env.PATROL_HORIZON_DAYS }}
        shell: bash
        run: |
          set -euo pipefail
          test -f "${CRIME_DATA_DIR}/risk_hourly.parquet" || { echo "❌ risk_hourly.parquet bulunamadı"; exit 2; }
          python -u scripts/fetch_weather_fcst.py || echo "⚠ forecast fetch optional"
          python -u scripts/post_patrol.py --input-parquet "${CRIME_DATA_DIR}/risk_hourly.parquet" --output-parquet "${CRIME_DATA_DIR}/patrol_recs_multi.parquet"
          test -f "${CRIME_DATA_DIR}/patrol_recs_multi.parquet" || { echo "❌ patrol çıktı yok"; exit 2; }

      - name: 15) Quick preview (parquet outputs)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        shell: bash
        run: |
          python - <<'PY'
          import os, polars as pl
          d=os.environ["CRIME_DATA_DIR"]
          for p in [
            "sf_crime_grid_full_labeled.parquet",
            "risk_hourly.parquet","patrol_recs_multi.parquet",
            "sf_crime_05_y.parquet","sf_crime_06_y.parquet","sf_crime_07_y.parquet","sf_crime_08_y.parquet",
            "sf_crime_y.parquet","sf_crime_01_y.parquet","sf_crime_02_y.parquet","sf_crime_03_y.parquet","sf_population_y.parquet"
          ]:
              f=os.path.join(d,p)
              if os.path.exists(f):
                  print("=== ",p," ===")
                  try:
                      print(pl.scan_parquet(f).head(5).collect())
                  except Exception as e:
                      print("preview error:", e)
          PY

      # ---- ÇIKTILAR ----
      - name: Zip parquet outputs into single artifact
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          ts=$(date +%Y%m%d)
          mkdir -p upload
          zip -r upload/sf-crime-parquet-${ts}.zip "${CRIME_DATA_DIR}"/*.parquet "${CRIME_DATA_DIR}"/sf_forecast_cat || true
          ls -lah upload || true

      - name: Upload artifact (single zip + loose files)
        if: ${{ steps.gate.outputs.proceed == 'true' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'artifact')) }}
        uses: actions/upload-artifact@v4
        with:
          name: sf-crime-pipeline-output
          path: |
            upload/*.zip
            ${{ env.CRIME_DATA_DIR }}/*.parquet
            ${{ env.CRIME_DATA_DIR }}/sf_forecast_cat/**
            ${{ env.CRIME_DATA_DIR }}/*.geojson
            ${{ env.CRIME_DATA_DIR }}/metrics_*.csv
            ${{ env.CRIME_DATA_DIR }}/models/*.joblib
            ${{ env.CRIME_DATA_DIR }}/oof_base_probs*.npz
          if-no-files-found: warn
          retention-days: 14

      - name: Commit & push (opsiyonel)
        if: ${{ steps.gate.outputs.proceed == 'true' && github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'commit' }}
        run: |
          set -e
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "🔄 Full pipeline (ID-based + Parquet + _y priority + rolling artifact) [skip ci]" || echo "No changes"
          git push || echo "Nothing to push"

      - name: Notify success (SendGrid HTTP)
        if: ${{ steps.gate.outputs.proceed == 'true' && success() }}
        env:
          SG_KEY: ${{ secrets.SENDGRID_API_KEY }}
        run: |
          if [ -z "$SG_KEY" ]; then echo "❌ SENDGRID_API_KEY yok/boş"; exit 1; fi
          curl -s -X POST https://api.sendgrid.com/v3/mail/send \
            -H "Authorization: Bearer $SG_KEY" \
            -H "Content-Type: application/json" \
            -d @- <<'EOF' -o /dev/stderr -w "\nHTTP:%{http_code}\n"
          {
            "personalizations":[ {"to":[ {"email":"cem5113@hotmail.com"}, {"email":"cemeroglu5113@gmail.com"} ]} ],
            "from":{"email":"${{ secrets.SG_FROM }}","name":"SF Crime Pipeline"},
            "reply_to":{"email":"cem5113@hotmail.com","name":"Cem"},
            "subject":"✅ SF Crime Pipeline başarı (_y öncelik + Parquet + Forecast) #${{ github.run_number }}",
            "categories":["sf-crime-pipeline","success"],
            "content":[{"type":"text/html","value":"<p>Pipeline başarıyla tamamlandı.</p><ul><li>ID-based + Parquet + _y öncelik</li><li>Rolling <code>_y.parquet</code> güncellendi</li><li>Category Forecast hazır</li><li>Run #: <b>${{ github.run_number }}</b></li></ul><p><a href='${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}'>Log & Artifact</a></p>"}]
          }
          EOF

      - name: Notify failure (SendGrid HTTP)
        if: ${{ steps.gate.outputs.proceed == 'true' && failure() }}
        env:
          SG_KEY: ${{ secrets.SENDGRID_API_KEY }}
        run: |
          if [ -z "$SG_KEY" ]; then echo "❌ SENDGRID_API_KEY yok/boş"; exit 1; fi
          curl -s -X POST https://api.sendgrid.com/v3/mail/send \
            -H "Authorization: Bearer $SG_KEY" \
            -H "Content-Type: application/json" \
            -d @- <<'EOF' -o /dev/stderr -w "\nHTTP:%{http_code}\n"
          {
            "personalizations":[ {"to":[ {"email":"cem5113@hotmail.com"}, {"email":"cemeroglu5113@gmail.com"} ]} ],
            "from":{"email":"${{ secrets.SG_FROM }}","name":"SF Crime Pipeline"},
            "reply_to":{"email":"cem5113@hotmail.com","name":"Cem"},
            "subject":"❌ SF Crime Pipeline hata (_y öncelik + Parquet) #${{ github.run_number }}",
            "categories":["sf-crime-pipeline","failure"],
            "content":[{"type":"text/html","value":"<p>Pipeline başarısız.</p><ul><li>Run #: <b>${{ github.run_number }}</b></li><li>ID-based + Parquet + _y modunda hata</li></ul><p><a href='${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}'>Log</a></p>"}]
          }
          EOF

      - name: Write run summary (end)
        if: ${{ steps.gate.outputs.proceed == 'true' && success() }}
        run: |
          {
            echo "## Tamamlandı"
            echo "- Üretilen başlıca parquetler:"
            echo "  - sf_crime_y.parquet (rolling union, Y_label=0/1)"
            echo "  - sf_crime_grid_full_labeled.parquet (ID-based grid + Y_label)"
            echo "  - sf_crime_01_y.parquet … sf_crime_08_y.parquet"
            echo "  - risk_hourly.parquet, patrol_recs_multi.parquet"
            echo "  - sf_forecast_cat/ (category partitioned, horizons: ${HORIZONS})"
            echo "- Tek zip artifact: upload/sf-crime-parquet-$(date +%Y%m%d).zip"
          } >> $GITHUB_STEP_SUMMARY
